{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from utils import *\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation, Lambda\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.initializers import glorot_uniform\n",
    "\n",
    "X_train, Y_train = read_csv('data/train.csv') \n",
    "X_test, Y_test = read_csv('data/test.csv') \n",
    "\n",
    "maxLen = 15\n",
    "print(maxLen)\n",
    "\n",
    "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('data/glove.6B.50d.txt')\n",
    "\n",
    "def sentences_to_indices(X, word_to_index, max_len):\n",
    "    \"\"\"\n",
    "    Converts an array of sentences (strings) into an array of indices corresponding to words in the sentences.\n",
    "    The output shape should be such that it can be given to `Embedding()` (described in Figure 4). \n",
    "    \n",
    "    Arguments:\n",
    "    X -- array of sentences (strings), of shape (m, 1)\n",
    "    word_to_index -- a dictionary containing the each word mapped to its index\n",
    "    max_len -- maximum number of words in a sentence. You can assume every sentence in X is no longer than this. \n",
    "    \n",
    "    Returns:\n",
    "    X_indices -- array of indices corresponding to words in the sentences from X, of shape (m, max_len)\n",
    "    \"\"\"\n",
    "\n",
    "    m = X.shape[0]\n",
    "    X_indices = np.zeros((m, max_len))\n",
    "    for i in range(m):\n",
    "        sentence_words = X[i].lower().split()\n",
    "        j = 0\n",
    "        for w in sentence_words:\n",
    "            if w not in word_to_index:\n",
    "                X_indices[i, j] = 0 # HACK - FIX SOON\n",
    "            else:\n",
    "                if j >= maxLen:\n",
    "                    print (sentence_words)\n",
    "                X_indices[i, j] = word_to_index[w]\n",
    "            j = j+1\n",
    "    return X_indices\n",
    "\n",
    "def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",
    "    \"\"\"\n",
    "    Creates a Keras Embedding() layer and loads in pre-trained GloVe 50-dimensional vectors.\n",
    "    \n",
    "    Arguments:\n",
    "    word_to_vec_map -- dictionary mapping words to their GloVe vector representation.\n",
    "    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)\n",
    "\n",
    "    Returns:\n",
    "    embedding_layer -- pretrained layer Keras instance\n",
    "    \"\"\"\n",
    "\n",
    "    vocab_len = len(word_to_index) + 1\n",
    "    emb_dim = word_to_vec_map[\"lemon\"].shape[0]\n",
    "    emb_matrix = np.zeros((vocab_len, emb_dim)) # curious why not transpose of this...\n",
    "    # Sets each row \"index\" of the embedding matrix to be \n",
    "    # the word vector representation of the \"index\"th word of the vocabulary\n",
    "    for word, index in word_to_index.items():\n",
    "        emb_matrix[index, :] = word_to_vec_map[word]\n",
    "    \n",
    "    embedding_layer = Embedding(vocab_len, emb_dim, trainable = False)\n",
    "\n",
    "    embedding_layer.build((None,))\n",
    "    embedding_layer.set_weights([emb_matrix]) # now it's pretrained!\n",
    "\n",
    "    return embedding_layer\n",
    "\n",
    "embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "\n",
    "def Model_V1(input_shape, word_to_vec_map, word_to_index):\n",
    "    \"\"\"\n",
    "    Function creating the Model-V1 model's graph.\n",
    "    \n",
    "    Arguments:\n",
    "    input_shape -- shape of the input, usually (max_len,)\n",
    "    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation\n",
    "    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)\n",
    "\n",
    "    Returns:\n",
    "    model -- a model instance in Keras\n",
    "    \"\"\"\n",
    "    \n",
    "    sentence_indices = Input(shape = input_shape, dtype = np.int32)\n",
    "    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "\n",
    "    # Propagates sentence_indices through the embedding layer\n",
    "    embeddings = embedding_layer(sentence_indices)\n",
    "\n",
    "    # Propagate the embeddings through an LSTM layer with 128-dimensional hidden state\n",
    "    LSTM1 = LSTM(128, return_sequences = True,name='LSTM1')(embeddings)\n",
    "    # Adds dropout with probability 0.5\n",
    "    X = Dropout(0.5)(LSTM1)\n",
    "    # Another LSTM layer, but just returns one output\n",
    "    LSTM2 = LSTM(128, return_sequences = True, name='LSTM2')(X)\n",
    "    \n",
    "    def get_last(X):\n",
    "        return X[:,-1,:]\n",
    "    \n",
    "    LSTM2Last = Lambda(get_last, name='LSTM2-last')(LSTM2)\n",
    "    Dropout2 = Dropout(0.5,name='Dropout2')(LSTM2Last)\n",
    "    \n",
    "    # Propagating through a Dense layer with sigmoid activation to get back a scalar\n",
    "    Dense1 = Dense(1,name='Dense1')(Dropout2)\n",
    "    X = Activation('sigmoid',name='output_layer')(Dense1)\n",
    "\n",
    "    model = Model(inputs = sentence_indices, outputs = X)\n",
    "\n",
    "    return model\n",
    "\n",
    "model = Model_V1((maxLen,), word_to_vec_map, word_to_index)\n",
    "\n",
    "# might want to change the metric here\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "X_train_indices = sentences_to_indices(X_train, word_to_index, maxLen)\n",
    "X_test_indices = sentences_to_indices(X_test, word_to_index, max_len = maxLen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 201 samples, validate on 184 samples\n",
      "Epoch 1/20\n",
      "201/201 [==============================] - 2s 10ms/step - loss: 0.6832 - acc: 0.5174 - val_loss: 0.5999 - val_acc: 0.7120\n",
      "Epoch 2/20\n",
      "201/201 [==============================] - 1s 5ms/step - loss: 0.5839 - acc: 0.7363 - val_loss: 0.5657 - val_acc: 0.7446\n",
      "Epoch 3/20\n",
      "201/201 [==============================] - 1s 5ms/step - loss: 0.5183 - acc: 0.7711 - val_loss: 0.3510 - val_acc: 0.8641\n",
      "Epoch 4/20\n",
      "201/201 [==============================] - 1s 5ms/step - loss: 0.4421 - acc: 0.8308 - val_loss: 0.3485 - val_acc: 0.8750\n",
      "Epoch 5/20\n",
      "201/201 [==============================] - 1s 5ms/step - loss: 0.4388 - acc: 0.8259 - val_loss: 0.3429 - val_acc: 0.8533\n",
      "Epoch 6/20\n",
      "201/201 [==============================] - 1s 5ms/step - loss: 0.4050 - acc: 0.8458 - val_loss: 0.3371 - val_acc: 0.8587\n",
      "Epoch 7/20\n",
      "201/201 [==============================] - 1s 5ms/step - loss: 0.3510 - acc: 0.8756 - val_loss: 0.4020 - val_acc: 0.8424\n",
      "Epoch 8/20\n",
      "201/201 [==============================] - 1s 4ms/step - loss: 0.3975 - acc: 0.8408 - val_loss: 0.3174 - val_acc: 0.8750\n",
      "Epoch 9/20\n",
      "201/201 [==============================] - 1s 5ms/step - loss: 0.3552 - acc: 0.8905 - val_loss: 0.5461 - val_acc: 0.7935\n",
      "Epoch 10/20\n",
      "201/201 [==============================] - 1s 5ms/step - loss: 0.3403 - acc: 0.8856 - val_loss: 0.3429 - val_acc: 0.8696\n",
      "Epoch 11/20\n",
      "201/201 [==============================] - 1s 5ms/step - loss: 0.2653 - acc: 0.9104 - val_loss: 0.4234 - val_acc: 0.8587\n",
      "Epoch 12/20\n",
      "201/201 [==============================] - 1s 5ms/step - loss: 0.2309 - acc: 0.9254 - val_loss: 0.3467 - val_acc: 0.8750\n",
      "Epoch 13/20\n",
      "201/201 [==============================] - 1s 5ms/step - loss: 0.1564 - acc: 0.9552 - val_loss: 0.3471 - val_acc: 0.8967\n",
      "Epoch 14/20\n",
      "201/201 [==============================] - 1s 4ms/step - loss: 0.1815 - acc: 0.9453 - val_loss: 0.7177 - val_acc: 0.7826\n",
      "Epoch 15/20\n",
      "201/201 [==============================] - 1s 5ms/step - loss: 0.2545 - acc: 0.9154 - val_loss: 0.3201 - val_acc: 0.8859\n",
      "Epoch 16/20\n",
      "201/201 [==============================] - 1s 5ms/step - loss: 0.1443 - acc: 0.9502 - val_loss: 0.4575 - val_acc: 0.8641\n",
      "Epoch 17/20\n",
      "201/201 [==============================] - 1s 5ms/step - loss: 0.2653 - acc: 0.9104 - val_loss: 0.3504 - val_acc: 0.8804\n",
      "Epoch 18/20\n",
      "201/201 [==============================] - 1s 5ms/step - loss: 0.1467 - acc: 0.9502 - val_loss: 0.4030 - val_acc: 0.8804\n",
      "Epoch 19/20\n",
      "201/201 [==============================] - 1s 4ms/step - loss: 0.1104 - acc: 0.9502 - val_loss: 0.5610 - val_acc: 0.8478\n",
      "Epoch 20/20\n",
      "201/201 [==============================] - 1s 4ms/step - loss: 0.0489 - acc: 0.9900 - val_loss: 0.6451 - val_acc: 0.8641\n",
      "1840/1840 [==============================] - 0s 254us/step\n",
      "\n",
      "Test accuracy =  0.8592391304347826\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "model.fit(X_train_indices, Y_train, epochs = 20, batch_size = 6, shuffle=True,validation_data=(X_dev_indices, Y_dev))\n",
    "loss, acc = model.evaluate(X_test_indices, Y_test)\n",
    "model.save('my_model.h5')\n",
    "print()\n",
    "print(\"Test accuracy = \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.engine import Input, Model, InputSpec\n",
    "from keras.layers import Dense, Activation, Dropout, Lambda\n",
    "from keras.layers import Embedding, LSTM\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.datasets import imdb\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def visualize_model_bs(model, include_gradients=False):\n",
    "\n",
    "    LSTM2 = model.get_layer('LSTM2')\n",
    "    Dropout2 = model.get_layer('Dropout2')\n",
    "    output_layer = model.get_layer('output_layer')\n",
    "\n",
    "    inputs = []\n",
    "    inputs.extend(model.inputs)\n",
    "\n",
    "    outputs = []\n",
    "    outputs.extend(model.outputs)\n",
    "    outputs.append(LSTM2.output)\n",
    "    outputs.append(LSTM2.cell.kernel_f)  # -- weights of the forget gates (assuming LSTM)\n",
    "    #print (LSTM1.trainable_weights)\n",
    "\n",
    "    if include_gradients:\n",
    "        loss = K.mean(model.output)  # [batch_size, 1] -> scalar\n",
    "        grads = K.gradients(loss, LSTM2.output)\n",
    "        grads_norm = grads / (K.sqrt(K.mean(K.square(grads))) + 1e-5)\n",
    "        outputs.append(grads_norm)\n",
    "\n",
    "    all_function = K.function(inputs, outputs)\n",
    "    output_function = K.function([Dropout2.input], model.outputs)\n",
    "    print(Dropout2.input)\n",
    "    return all_function, output_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"LSTM2-last_5/strided_slice:0\", shape=(?, 128), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "all_function, output_function = visualize_model_bs(model, include_gradients=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1) (1, 15, 128) (128, 128) (1, 1, 15, 128)\n",
      "Scores: [[0.90563]]\n",
      "Time distributed (word-level) scores: [array([[0.55840904],\n",
      "       [0.6682497 ],\n",
      "       [0.7709293 ],\n",
      "       [0.8368673 ],\n",
      "       [0.871035  ],\n",
      "       [0.8879638 ],\n",
      "       [0.8953616 ],\n",
      "       [0.89667165],\n",
      "       [0.9008593 ],\n",
      "       [0.90035504],\n",
      "       [0.9031226 ],\n",
      "       [0.90380293],\n",
      "       [0.9046281 ],\n",
      "       [0.9052044 ],\n",
      "       [0.9056301 ]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "sentence=\"You do me wrong very have a of months immediate\"\n",
    "t = np.array([sentence])\n",
    "X = sentences_to_indices(t, word_to_index, maxLen)\n",
    "# -- Return scores, raw rnn values and gradients\n",
    "# scores is equivalent to model.predict(X)\n",
    "scores, rnn_values, rnn_gradients, W_i = all_function([X])\n",
    "print(scores.shape, rnn_values.shape, rnn_gradients.shape, W_i.shape)\n",
    "\n",
    "# -- score prediction\n",
    "print(\"Scores:\", scores)\n",
    "\n",
    "# -- Return scores at each step in the time sequence\n",
    "time_distributed_scores = map(lambda x: output_function([x]), rnn_values)\n",
    "print(\"Time distributed (word-level) scores:\", map(lambda x: x[0], time_distributed_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[48;5;126m very\n",
      "\u001b[48;5;127m have\n",
      "\u001b[48;5;127m a\n",
      "\u001b[48;5;126m of\n",
      "\u001b[48;5;126m months\n",
      "\u001b[48;5;126m immediate\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "not all arguments converted during string formatting",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-179-75e8c553e206>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'%s %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbg\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'yes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: not all arguments converted during string formatting"
     ]
    }
   ],
   "source": [
    "from colored import fg, bg, attr\n",
    "\n",
    "words=sentence.split()\n",
    "base=124\n",
    "scale=6\n",
    "color_weight=base+np.array(time_distributed_scores).reshape(maxLen)*scale\n",
    "\n",
    "for i in range(len(words)):\n",
    "    print ('%s %s' % (bg (int(color_weight[i])),words[i]))\n",
    "\n",
    "for i in range(base,base+scale):\n",
    "    print ('%s %s' % (bg (i),attr(1),'yes'))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing dev/test/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# small \n",
    "X_test, Y_test = read_csv('data/test.csv') \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184\n"
     ]
    }
   ],
   "source": [
    "# small\n",
    "import pandas as pd\n",
    "dev_ratio=0.1\n",
    "total_test_num=len(X_test_indices)\n",
    "dev_num = int(dev_ratio * len(X_test_indices))\n",
    "X_dev_indices = []\n",
    "print (dev_num)\n",
    "dev_index=(np.random.random([dev_num])*total_test_num).astype(int)\n",
    "\n",
    "X_dev_indices=X_test_indices[dev_index,:]\n",
    "X_dev = X_test[dev_index]\n",
    "Y_dev = Y_test[dev_index]\n",
    "\n",
    "X_test_after_dev=np.delete(X_test,dev_index,0)\n",
    "Y_test_after_dev=np.delete(Y_test,dev_index,0)\n",
    "\n",
    "# test after dev\n",
    "test_after_dev={'X': X_test_after_dev, 'Y': Y_test_after_dev}\n",
    "test_after_dev = pd.DataFrame(test_after_dev)\n",
    "test_after_dev.to_csv('test_minus_dev.csv',header=False,index=False)\n",
    "\n",
    "# dev\n",
    "dev={'X': X_dev, 'Y': Y_dev}\n",
    "dev = pd.DataFrame(dev)\n",
    "dev.to_csv('dev.csv',header=False,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184\n"
     ]
    }
   ],
   "source": [
    "# big\n",
    "X_test, Y_test = read_csv('data/test-big.csv')\n",
    "\n",
    "# small\n",
    "import pandas as pd\n",
    "dev_ratio=0.1\n",
    "total_test_num=len(X_test_indices)\n",
    "dev_num = int(dev_ratio * len(X_test_indices))\n",
    "X_dev_indices = []\n",
    "print (dev_num)\n",
    "dev_index=(np.random.random([dev_num])*total_test_num).astype(int)\n",
    "\n",
    "X_dev_indices=X_test_indices[dev_index,:]\n",
    "X_dev = X_test[dev_index]\n",
    "Y_dev = Y_test[dev_index]\n",
    "\n",
    "X_test_after_dev=np.delete(X_test,dev_index,0)\n",
    "Y_test_after_dev=np.delete(Y_test,dev_index,0)\n",
    "\n",
    "# test after dev\n",
    "test_after_dev={'X': X_test_after_dev, 'Y': Y_test_after_dev}\n",
    "test_after_dev = pd.DataFrame(test_after_dev)\n",
    "test_after_dev.to_csv('test_minus_dev_big.csv',header=False,index=False)\n",
    "\n",
    "# dev\n",
    "dev={'X': X_dev, 'Y': Y_dev}\n",
    "dev = pd.DataFrame(dev)\n",
    "dev.to_csv('dev_big.csv',header=False,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
