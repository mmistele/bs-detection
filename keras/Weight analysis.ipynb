{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "201/201 [==============================] - 3s 17ms/step - loss: 0.6932 - acc: 0.5423\n",
      "Epoch 2/20\n",
      "201/201 [==============================] - 1s 5ms/step - loss: 0.5596 - acc: 0.7413\n",
      "Epoch 3/20\n",
      "201/201 [==============================] - 1s 5ms/step - loss: 0.5174 - acc: 0.7960\n",
      "Epoch 4/20\n",
      "201/201 [==============================] - 1s 5ms/step - loss: 0.4951 - acc: 0.8060\n",
      "Epoch 5/20\n",
      "201/201 [==============================] - 1s 5ms/step - loss: 0.4104 - acc: 0.8507\n",
      "Epoch 6/20\n",
      "201/201 [==============================] - 1s 5ms/step - loss: 0.4396 - acc: 0.8259\n",
      "Epoch 7/20\n",
      "201/201 [==============================] - 1s 5ms/step - loss: 0.4423 - acc: 0.8557\n",
      "Epoch 8/20\n",
      "201/201 [==============================] - 1s 7ms/step - loss: 0.3996 - acc: 0.8607\n",
      "Epoch 9/20\n",
      "201/201 [==============================] - 1s 6ms/step - loss: 0.4191 - acc: 0.8507\n",
      "Epoch 10/20\n",
      "201/201 [==============================] - 1s 5ms/step - loss: 0.3780 - acc: 0.8706\n",
      "Epoch 11/20\n",
      "201/201 [==============================] - 1s 5ms/step - loss: 0.3613 - acc: 0.8756\n",
      "Epoch 12/20\n",
      "201/201 [==============================] - 1s 5ms/step - loss: 0.3172 - acc: 0.8955\n",
      "Epoch 13/20\n",
      "201/201 [==============================] - 1s 5ms/step - loss: 0.3293 - acc: 0.8955\n",
      "Epoch 14/20\n",
      "201/201 [==============================] - 1s 5ms/step - loss: 0.3000 - acc: 0.9104\n",
      "Epoch 15/20\n",
      "201/201 [==============================] - 1s 5ms/step - loss: 0.2917 - acc: 0.9104\n",
      "Epoch 16/20\n",
      "201/201 [==============================] - 1s 5ms/step - loss: 0.3170 - acc: 0.9005\n",
      "Epoch 17/20\n",
      "201/201 [==============================] - 1s 5ms/step - loss: 0.2861 - acc: 0.9104\n",
      "Epoch 18/20\n",
      "201/201 [==============================] - 1s 6ms/step - loss: 0.2857 - acc: 0.9154\n",
      "Epoch 19/20\n",
      "201/201 [==============================] - 1s 5ms/step - loss: 0.4016 - acc: 0.8408\n",
      "Epoch 20/20\n",
      "201/201 [==============================] - 1s 6ms/step - loss: 0.4078 - acc: 0.8259\n",
      "1840/1840 [==============================] - 2s 830us/step\n",
      "\n",
      "Test accuracy =  0.8538043478260869\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from utils import *\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.initializers import glorot_uniform\n",
    "\n",
    "X_train, Y_train = read_csv('data/train.csv') \n",
    "X_test, Y_test = read_csv('data/test.csv') \n",
    "\n",
    "maxLen = len(max(X_train, key=len).split())\n",
    "\n",
    "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('data/glove.6B.50d.txt')\n",
    "\n",
    "def sentences_to_indices(X, word_to_index, max_len):\n",
    "    \"\"\"\n",
    "    Converts an array of sentences (strings) into an array of indices corresponding to words in the sentences.\n",
    "    The output shape should be such that it can be given to `Embedding()` (described in Figure 4). \n",
    "    \n",
    "    Arguments:\n",
    "    X -- array of sentences (strings), of shape (m, 1)\n",
    "    word_to_index -- a dictionary containing the each word mapped to its index\n",
    "    max_len -- maximum number of words in a sentence. You can assume every sentence in X is no longer than this. \n",
    "    \n",
    "    Returns:\n",
    "    X_indices -- array of indices corresponding to words in the sentences from X, of shape (m, max_len)\n",
    "    \"\"\"\n",
    "\n",
    "    m = X.shape[0]\n",
    "    X_indices = np.zeros((m, max_len))\n",
    "    for i in range(m):\n",
    "        sentence_words = X[i].lower().split()\n",
    "        j = 0\n",
    "        for w in sentence_words:\n",
    "            if w not in word_to_index:\n",
    "                X_indices[i, j] = 0 # HACK - FIX SOON\n",
    "            else:\n",
    "                X_indices[i, j] = word_to_index[w]\n",
    "            j = j+1\n",
    "    return X_indices\n",
    "\n",
    "def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",
    "    \"\"\"\n",
    "    Creates a Keras Embedding() layer and loads in pre-trained GloVe 50-dimensional vectors.\n",
    "    \n",
    "    Arguments:\n",
    "    word_to_vec_map -- dictionary mapping words to their GloVe vector representation.\n",
    "    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)\n",
    "\n",
    "    Returns:\n",
    "    embedding_layer -- pretrained layer Keras instance\n",
    "    \"\"\"\n",
    "\n",
    "    vocab_len = len(word_to_index) + 1\n",
    "    emb_dim = word_to_vec_map[\"lemon\"].shape[0]\n",
    "    emb_matrix = np.zeros((vocab_len, emb_dim)) # curious why not transpose of this...\n",
    "    # Sets each row \"index\" of the embedding matrix to be \n",
    "    # the word vector representation of the \"index\"th word of the vocabulary\n",
    "    for word, index in word_to_index.items():\n",
    "        emb_matrix[index, :] = word_to_vec_map[word]\n",
    "    \n",
    "    embedding_layer = Embedding(vocab_len, emb_dim, trainable = False)\n",
    "\n",
    "    embedding_layer.build((None,))\n",
    "    embedding_layer.set_weights([emb_matrix]) # now it's pretrained!\n",
    "\n",
    "    return embedding_layer\n",
    "\n",
    "embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "\n",
    "def Model_V1(input_shape, word_to_vec_map, word_to_index):\n",
    "    \"\"\"\n",
    "    Function creating the Model-V1 model's graph.\n",
    "    \n",
    "    Arguments:\n",
    "    input_shape -- shape of the input, usually (max_len,)\n",
    "    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation\n",
    "    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)\n",
    "\n",
    "    Returns:\n",
    "    model -- a model instance in Keras\n",
    "    \"\"\"\n",
    "    \n",
    "    sentence_indices = Input(shape = input_shape, dtype = np.int32)\n",
    "    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "\n",
    "    # Propagates sentence_indices through the embedding layer\n",
    "    embeddings = embedding_layer(sentence_indices)\n",
    "\n",
    "    # Propagate the embeddings through an LSTM layer with 128-dimensional hidden state\n",
    "    LSTM1 = LSTM(128, return_sequences = True,name='LSTM1')(embeddings)\n",
    "    # Adds dropout with probability 0.5\n",
    "    X = Dropout(0.5)(LSTM1)\n",
    "    # Another LSTM layer, but just returns one output\n",
    "    LSTM2 = LSTM(128, return_sequences = True, name='LSTM2')(X)\n",
    "    \n",
    "    def get_last(X):\n",
    "        return X[:,-1,:]\n",
    "    \n",
    "    LSTM2Last = Lambda(get_last, name='LSTM2-last')(LSTM2)\n",
    "    Dropout2 = Dropout(0.5,name='Dropout2')(LSTM2Last)\n",
    "    \n",
    "    # Propagating through a Dense layer with sigmoid activation to get back a scalar\n",
    "    Dense1 = Dense(1,name='Dense1')(Dropout2)\n",
    "    X = Activation('sigmoid',name='output_layer')(Dense1)\n",
    "\n",
    "    model = Model(inputs = sentence_indices, outputs = X)\n",
    "\n",
    "    return model\n",
    "\n",
    "model = Model_V1((maxLen,), word_to_vec_map, word_to_index)\n",
    "\n",
    "# might want to change the metric here\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "X_train_indices = sentences_to_indices(X_train, word_to_index, maxLen)\n",
    "\n",
    "model.fit(X_train_indices, Y_train, epochs = 20, batch_size = 6, shuffle=True)\n",
    "\n",
    "X_test_indices = sentences_to_indices(X_test, word_to_index, max_len = maxLen)\n",
    "loss, acc = model.evaluate(X_test_indices, Y_test)\n",
    "model.save('my_model.h5')\n",
    "print()\n",
    "print(\"Test accuracy = \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def visualize_model_bs(model, include_gradients=False):\n",
    "\n",
    "    LSTM2 = model.get_layer('LSTM2')\n",
    "    Dropout2 = model.get_layer('Dropout2')\n",
    "    output_layer = model.get_layer('output_layer')\n",
    "\n",
    "    inputs = []\n",
    "    inputs.extend(model.inputs)\n",
    "\n",
    "    outputs = []\n",
    "    outputs.extend(model.outputs)\n",
    "    outputs.append(LSTM2.output)\n",
    "    outputs.append(LSTM2.cell.kernel_f)  # -- weights of the forget gates (assuming LSTM)\n",
    "    #print (LSTM1.trainable_weights)\n",
    "\n",
    "    if include_gradients:\n",
    "        loss = K.mean(model.output)  # [batch_size, 1] -> scalar\n",
    "        grads = K.gradients(loss, LSTM2.output)\n",
    "        grads_norm = grads / (K.sqrt(K.mean(K.square(grads))) + 1e-5)\n",
    "        outputs.append(grads_norm)\n",
    "\n",
    "    all_function = K.function(inputs, outputs)\n",
    "    output_function = K.function([Dropout2.input], model.outputs)\n",
    "    print(Dropout2.input)\n",
    "    return all_function, output_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"LSTM2-last_1/strided_slice:0\", shape=(?, 128), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "all_function, output_function = visualize_model_bs(model, include_gradients=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 1) (2, 22, 128) (128, 128) (1, 2, 22, 128)\n",
      "Scores: [[0.8317234 ]\n",
      " [0.09714699]]\n",
      "Time distributed (word-level) scores: [array([[0.5103478 ],\n",
      "       [0.5637954 ],\n",
      "       [0.6428501 ],\n",
      "       [0.7160077 ],\n",
      "       [0.7672935 ],\n",
      "       [0.79743946],\n",
      "       [0.81008464],\n",
      "       [0.82299113],\n",
      "       [0.8304071 ],\n",
      "       [0.83671296],\n",
      "       [0.83815575],\n",
      "       [0.8378455 ],\n",
      "       [0.8369964 ],\n",
      "       [0.83584464],\n",
      "       [0.83504534],\n",
      "       [0.8343906 ],\n",
      "       [0.8338339 ],\n",
      "       [0.8333432 ],\n",
      "       [0.83289826],\n",
      "       [0.83248544],\n",
      "       [0.83209586],\n",
      "       [0.8317234 ]], dtype=float32), array([[0.49421164],\n",
      "       [0.5000774 ],\n",
      "       [0.49013227],\n",
      "       [0.4685592 ],\n",
      "       [0.43677354],\n",
      "       [0.3991497 ],\n",
      "       [0.35915443],\n",
      "       [0.31949675],\n",
      "       [0.2821362 ],\n",
      "       [0.24833643],\n",
      "       [0.21876137],\n",
      "       [0.19359471],\n",
      "       [0.1726637 ],\n",
      "       [0.15556085],\n",
      "       [0.14175455],\n",
      "       [0.1306813 ],\n",
      "       [0.12181053],\n",
      "       [0.11468153],\n",
      "       [0.10891619],\n",
      "       [0.10421613],\n",
      "       [0.1003514 ],\n",
      "       [0.09714699]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "t = np.array([\"You need not to have called me up so late at night\",'As am is'])\n",
    "X = sentences_to_indices(t, word_to_index, maxLen)\n",
    "# -- Return scores, raw rnn values and gradients\n",
    "# scores is equivalent to model.predict(X)\n",
    "scores, rnn_values, rnn_gradients, W_i = all_function([X])\n",
    "print(scores.shape, rnn_values.shape, rnn_gradients.shape, W_i.shape)\n",
    "\n",
    "# -- score prediction\n",
    "print(\"Scores:\", scores)\n",
    "\n",
    "# -- Return scores at each step in the time sequence\n",
    "time_distributed_scores = map(lambda x: output_function([x]), rnn_values)\n",
    "print(\"Time distributed (word-level) scores:\", map(lambda x: x[0], time_distributed_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.00742034,  0.03337668,  0.01174635, ...,  0.0531809 ,\n",
       "          0.00585037,  0.05965093],\n",
       "        [ 0.04074103,  0.04676349,  0.03048183, ...,  0.06056848,\n",
       "          0.06381115,  0.09195539],\n",
       "        [ 0.10616807,  0.01623522,  0.0449459 , ...,  0.033544  ,\n",
       "          0.17325191,  0.09447686],\n",
       "        ...,\n",
       "        [ 0.21929474, -0.41297337,  0.06639624, ..., -0.07637198,\n",
       "          0.6058215 , -0.24190804],\n",
       "        [ 0.21930094, -0.41567263,  0.06757586, ..., -0.07720998,\n",
       "          0.60561544, -0.24450427],\n",
       "        [ 0.21928251, -0.41798908,  0.06862893, ..., -0.07794573,\n",
       "          0.605298  , -0.24689142]],\n",
       "\n",
       "       [[-0.02379844,  0.02844131, -0.01638879, ...,  0.01416098,\n",
       "         -0.02209499,  0.00391491],\n",
       "        [-0.01955068,  0.0694175 , -0.03651516, ...,  0.05530659,\n",
       "         -0.05460837,  0.04872531],\n",
       "        [-0.05751175,  0.12331344, -0.06953079, ...,  0.07751152,\n",
       "         -0.08808772,  0.08417251],\n",
       "        ...,\n",
       "        [-0.44136357,  0.27019277, -0.45307258, ...,  0.12415995,\n",
       "         -0.12804525,  0.08900017],\n",
       "        [-0.44530225,  0.2671444 , -0.4587645 , ...,  0.12484964,\n",
       "         -0.12613872,  0.08667993],\n",
       "        [-0.4481292 ,  0.26441488, -0.4637588 , ...,  0.12549537,\n",
       "         -0.12468018,  0.08458489]]], dtype=float32)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight analysis\n",
    "-> need to check papers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neuron Firing (example based)\n",
    "-> given two inputs (correct, wrong) \n",
    "\n",
    "-> check how the neron fire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 4ms/step\n",
      "1.5999397039413452\n"
     ]
    }
   ],
   "source": [
    "x1=\"I had breakfirst today\"\n",
    "x2=\"breakfirst had today I\"\n",
    "y1=1\n",
    "y2=0\n",
    "\n",
    "x_ana_indices = sentences_to_indices(np.array([x1,x2]), word_to_index, maxLen)\n",
    "\n",
    "loss, acc=model.evaluate(x_ana_indices, [y1,y2])\n",
    "print loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence='I like the color today'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## error analysis\n",
    "-> dev, testing, training error rate, and analysis how to improve the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "X_train shape: (25000, 40)\n",
      "X_test shape: (25000, 40)\n",
      "Compiling...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_layer (InputLayer)     (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "embedding_layer (Embedding)  (None, 40, 32)            2834688   \n",
      "_________________________________________________________________\n",
      "recurrent_layer (LSTM)       (None, 40, 32)            8320      \n",
      "_________________________________________________________________\n",
      "last_step_layer (Lambda)     (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "output_layer (Dense)         (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 2,843,041\n",
      "Trainable params: 2,843,041\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/__main__.py:66: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(units=32, return_sequences=True, activation=\"tanh\", name=\"recurrent_layer\")`\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/__main__.py:71: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"ou..., inputs=Tensor(\"in...)`\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/__main__.py:84: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.6932 - val_loss: 0.6917\n",
      "1000/1000 [==============================] - 0s 167us/step\n",
      "Test accuracy: 0.6917306571006775\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.engine import Input, Model, InputSpec\n",
    "from keras.layers import Dense, Activation, Dropout, Lambda\n",
    "from keras.layers import Embedding, LSTM\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.datasets import imdb\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import pdb\n",
    "\n",
    "MAX_FEATURES = 20000\n",
    "MAXLEN = 40\n",
    "BATCH_SIZE = 32\n",
    "STEP = 3\n",
    "EMBEDDING_DIM = 32\n",
    "RNN_HIDDEN_DIM = 32\n",
    "\n",
    "# TODO: add normalization\n",
    "# TODO: activation cluster?\n",
    "# TODO: get forget gates from LSTMs (not sure how to grab temporary tensors inside loops)\n",
    "\n",
    "\n",
    "def build_vocab():\n",
    "    vocab = imdb.get_word_index()\n",
    "    rev_vocab = {v: k for k, v in vocab.iteritems()}\n",
    "    return vocab, rev_vocab\n",
    "\n",
    "\n",
    "def vectorize(text, vocab, \n",
    "              maxlen=MAXLEN, start_char=1, oov_char=2, index_from=3):\n",
    "    \"\"\" might not be consistent with vectorize_data. \"\"\"\n",
    "    if isinstance(text, basestring):\n",
    "        text = [text]\n",
    "    v = [[vocab.get(w, oov_char) for w in t.lower().split()] for t in text]\n",
    "    return sequence.pad_sequences(v, maxlen=maxlen)\n",
    "\n",
    "\n",
    "def vectorize_data(max_features=MAX_FEATURES, maxlen=MAXLEN, batch_size=BATCH_SIZE, limit=None):\n",
    "\n",
    "    print('Loading data...')\n",
    "    (X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=max_features)\n",
    "    print(len(X_train), 'train sequences')\n",
    "    print(len(X_test), 'test sequences')\n",
    "\n",
    "    print('Pad sequences (samples x time)')\n",
    "    X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "    X_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "    print('X_train shape:', X_train.shape)\n",
    "    print('X_test shape:', X_test.shape)\n",
    "\n",
    "    if limit is None:\n",
    "        return X_train, y_train, X_test, y_test\n",
    "    else:\n",
    "        return X_train[:limit], y_train[:limit], X_test[:limit], y_test[:limit]\n",
    "\n",
    "\n",
    "def build_model(vocab_size, maxlen=MAXLEN, rnn_hidden_dim=RNN_HIDDEN_DIM):\n",
    "    input_layer = Input(shape=(maxlen, ), name='input_layer', dtype='int32')\n",
    "    embedding_layer = Embedding(vocab_size, EMBEDDING_DIM, input_length=maxlen, trainable=True, mask_zero=False, name='embedding_layer')(input_layer)  # -- ideally mask_zero=False but can't work with lambda layers\n",
    "    recurrent_layer = LSTM(output_dim=rnn_hidden_dim, activation='tanh', return_sequences=True, name='recurrent_layer')(embedding_layer)\n",
    "    last_step_layer = Lambda(lambda x: x[:, -1, :], output_shape=(rnn_hidden_dim, ), name='last_step_layer')(recurrent_layer)  # only needed for visualization\n",
    "    output_layer = Dense(1, activation='sigmoid', name='output_layer')(last_step_layer)\n",
    "\n",
    "    optimizer = Adam(lr=0.001)\n",
    "    model = Model(input=input_layer, output=output_layer)\n",
    "    print('Compiling...')\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # -- train\n",
    "    vocab, rev_vocab = build_vocab()\n",
    "    X_train, y_train, X_test, y_test = vectorize_data(limit=1000)\n",
    "    model = build_model(len(vocab))\n",
    "    model.fit(X_train, y_train, batch_size=BATCH_SIZE, nb_epoch=1, verbose=True, # validation_split=0.05)\n",
    "              validation_data=(X_test, y_test))\n",
    "\n",
    "    acc = model.evaluate(X_test, y_test, batch_size=BATCH_SIZE)\n",
    "    print('Test accuracy:', acc)\n",
    "\n",
    "    # -- predict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1) (1, 40, 32) (32, 32) (1, 1, 40, 32)\n",
      "Scores: [[0.498088]]\n",
      "Time distributed (word-level) scores: [array([[0.5006162 ],\n",
      "       [0.5007734 ],\n",
      "       [0.5008125 ],\n",
      "       [0.5007444 ],\n",
      "       [0.50059927],\n",
      "       [0.50040865],\n",
      "       [0.5001987 ],\n",
      "       [0.49998844],\n",
      "       [0.49979004],\n",
      "       [0.49961036],\n",
      "       [0.49945253],\n",
      "       [0.49931702],\n",
      "       [0.49920276],\n",
      "       [0.49910778],\n",
      "       [0.49902967],\n",
      "       [0.49896622],\n",
      "       [0.49891493],\n",
      "       [0.49887398],\n",
      "       [0.49884135],\n",
      "       [0.4988156 ],\n",
      "       [0.49879533],\n",
      "       [0.49877948],\n",
      "       [0.4987672 ],\n",
      "       [0.49875766],\n",
      "       [0.49875036],\n",
      "       [0.4987448 ],\n",
      "       [0.49874052],\n",
      "       [0.4987373 ],\n",
      "       [0.49873495],\n",
      "       [0.49873316],\n",
      "       [0.49873185],\n",
      "       [0.4987309 ],\n",
      "       [0.49495125],\n",
      "       [0.49456042],\n",
      "       [0.4971958 ],\n",
      "       [0.49947283],\n",
      "       [0.49798134],\n",
      "       [0.49419206],\n",
      "       [0.49576503],\n",
      "       [0.498088  ]], dtype=float32)]\n",
      "--Call--\n",
      "> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/IPython/core/displayhook.py(236)__call__()\n",
      "-> def __call__(self, result=None):\n",
      "(Pdb) 1\n",
      "1\n",
      "(Pdb) 2\n",
      "2\n",
      "(Pdb) 3\n",
      "3\n",
      "(Pdb) 4\n",
      "4\n",
      "(Pdb) 5\n",
      "5\n",
      "(Pdb) 6\n",
      "6\n",
      "(Pdb) 7\n",
      "7\n",
      "(Pdb) 8\n",
      "8\n",
      "(Pdb) 9\n",
      "9\n",
      "(Pdb) 0\n",
      "0\n",
      "(Pdb) 123\n",
      "123\n",
      "(Pdb) s\n",
      "> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/IPython/core/displayhook.py(242)__call__()\n",
      "-> self.check_for_underscore()\n",
      "(Pdb) d\n",
      "*** Newest frame\n",
      "(Pdb) f\n",
      "*** NameError: name 'f' is not defined\n",
      "(Pdb) g\n",
      "*** NameError: name 'g' is not defined\n",
      "(Pdb) h\n",
      "\n",
      "Documented commands (type help <topic>):\n",
      "========================================\n",
      "EOF    bt         cont      enable  jump  pp       run      unt   \n",
      "a      c          continue  exit    l     q        s        until \n",
      "alias  cl         d         h       list  quit     step     up    \n",
      "args   clear      debug     help    n     r        tbreak   w     \n",
      "b      commands   disable   ignore  next  restart  u        whatis\n",
      "break  condition  down      j       p     return   unalias  where \n",
      "\n",
      "Miscellaneous help topics:\n",
      "==========================\n",
      "exec  pdb\n",
      "\n",
      "Undocumented commands:\n",
      "======================\n",
      "retval  rv\n",
      "\n",
      "(Pdb) j\n",
      "*** The 'jump' command requires a line number.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-97-b67627f2809f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Time distributed (word-level) scores:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_distributed_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/IPython/core/displayhook.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, result)\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0mactivated\u001b[0m \u001b[0mby\u001b[0m \u001b[0msetting\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mvariable\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplayhook\u001b[0m \u001b[0mto\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m         \"\"\"\n\u001b[0;32m--> 242\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_for_underscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquiet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_displayhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/IPython/core/displayhook.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, result)\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0mactivated\u001b[0m \u001b[0mby\u001b[0m \u001b[0msetting\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mvariable\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplayhook\u001b[0m \u001b[0mto\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m         \"\"\"\n\u001b[0;32m--> 242\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_for_underscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquiet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_displayhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/bdb.pyc\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;31m# None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'line'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'call'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/bdb.pyc\u001b[0m in \u001b[0;36mdispatch_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdispatch_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pdb.pyc\u001b[0m in \u001b[0;36muser_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_mainpyfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbp_commands\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minteraction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbp_commands\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pdb.pyc\u001b[0m in \u001b[0;36minteraction\u001b[0;34m(self, frame, traceback)\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_stack_entry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcmdloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/cmd.pyc\u001b[0m in \u001b[0;36mcmdloop\u001b[0;34m(self, intro)\u001b[0m\n\u001b[1;32m    128\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_rawinput\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m                             \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m                         \u001b[0;32mexcept\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m                             \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'EOF'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/kernelbase.pyc\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    687\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 689\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    690\u001b[0m         )\n\u001b[1;32m    691\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/kernelbase.pyc\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    717\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "all_function, output_function = visualize_model(model, include_gradients=True)\n",
    "\n",
    "t = \"HOW COULD anything originate out of its opposite?\".lower()\n",
    "X = vectorize(t, vocab)\n",
    "\n",
    "# -- Return scores, raw rnn values and gradients\n",
    "# scores is equivalent to model.predict(X)\n",
    "scores, rnn_values, rnn_gradients, W_i = all_function([X])\n",
    "print(scores.shape, rnn_values.shape, rnn_gradients.shape, W_i.shape)\n",
    "\n",
    "# -- score prediction\n",
    "print(\"Scores:\", scores)\n",
    "\n",
    "# -- Return scores at each step in the time sequence\n",
    "time_distributed_scores = map(lambda x: output_function([x]), rnn_values)\n",
    "print(\"Time distributed (word-level) scores:\", map(lambda x: x[0], time_distributed_scores))\n",
    "\n",
    "pdb.set_trace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def visualize_model(model, include_gradients=False):\n",
    "    recurrent_layer = model.get_layer('recurrent_layer')\n",
    "    output_layer = model.get_layer('output_layer')\n",
    "\n",
    "    inputs = []\n",
    "    inputs.extend(model.inputs)\n",
    "\n",
    "    outputs = []\n",
    "    outputs.extend(model.outputs)\n",
    "    outputs.append(recurrent_layer.output)\n",
    "    outputs.append(recurrent_layer.cell.kernel_f)  # -- weights of the forget gates (assuming LSTM)\n",
    "\n",
    "    if include_gradients:\n",
    "        loss = K.mean(model.output)  # [batch_size, 1] -> scalar\n",
    "        grads = K.gradients(loss, recurrent_layer.output)\n",
    "        grads_norm = grads / (K.sqrt(K.mean(K.square(grads))) + 1e-5)\n",
    "        outputs.append(grads_norm)\n",
    "\n",
    "    all_function = K.function(inputs, outputs)\n",
    "    output_function = K.function([output_layer.input], model.outputs)\n",
    "    return all_function, output_function\n",
    "\n",
    "def get_compare_embeddings(original_embeddings, tuned_embeddings, vocab, dimreduce_type=\"pca\", random_state=0):\n",
    "    \"\"\" Compare embeddings drift. \"\"\"\n",
    "    if dimreduce_type == \"pca\":\n",
    "        from sklearn.decomposition import PCA\n",
    "        dimreducer = PCA(n_components=2, random_state=random_state)\n",
    "    elif dimreduce_type == \"tsne\":\n",
    "        from sklearn.manifold import TSNE\n",
    "        dimreducer = TSNE(n_components=2, random_state=random_state)\n",
    "    else:\n",
    "        raise Exception(\"Wrong dimreduce_type.\")\n",
    "\n",
    "    reduced_original = dimreducer.fit_transform(original_embeddings)\n",
    "    reduced_tuned = dimreducer.fit_transform(tuned_embeddings)\n",
    "\n",
    "    def compare_embeddings(word):\n",
    "        if word not in vocab:\n",
    "            return None\n",
    "        word_id = vocab[word]\n",
    "        original_x, original_y = reduced_original[word_id, :]\n",
    "        tuned_x, tuned_y = reduced_tuned[word_id, :]\n",
    "        return original_x, original_y, tuned_x, tuned_y\n",
    "\n",
    "    return compare_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
