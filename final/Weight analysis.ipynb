{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils import *\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation, Lambda,Bidirectional\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.initializers import glorot_uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9908\n",
      "33\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train = read_csv('data/train-big.csv') \n",
    "X_test, Y_test = read_csv('data/test_minus_dev_big.csv') \n",
    "# small \n",
    "X_dev, Y_dev = read_csv('data/dev_big.csv')\n",
    "\n",
    "\n",
    "ratio=0.1\n",
    "total_train_num=len(X_train)\n",
    "train_num = int(ratio * total_train_num)\n",
    "train_index=(np.random.random([train_num])*total_train_num).astype(int)\n",
    "print (train_num)\n",
    "\n",
    "X_train=X_train[train_index]\n",
    "Y_train = Y_train[train_index]\n",
    "\n",
    "\n",
    "maxLen = len(max(X_train, key=len).split())+10\n",
    "print(maxLen)\n",
    "\n",
    "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('data/glove.6B.50d.txt')\n",
    "\n",
    "def sentences_to_indices(X, word_to_index, max_len):\n",
    "    \"\"\"\n",
    "    Converts an array of sentences (strings) into an array of indices corresponding to words in the sentences.\n",
    "    The output shape should be such that it can be given to `Embedding()` (described in Figure 4). \n",
    "    \n",
    "    Arguments:\n",
    "    X -- array of sentences (strings), of shape (m, 1)\n",
    "    word_to_index -- a dictionary containing the each word mapped to its index\n",
    "    max_len -- maximum number of words in a sentence. You can assume every sentence in X is no longer than this. \n",
    "    \n",
    "    Returns:\n",
    "    X_indices -- array of indices corresponding to words in the sentences from X, of shape (m, max_len)\n",
    "    \"\"\"\n",
    "\n",
    "    m = X.shape[0]\n",
    "    X_indices = np.zeros((m, max_len))\n",
    "    for i in range(m):\n",
    "        sentence_words = X[i].lower().split()\n",
    "        j = 0\n",
    "        for w in sentence_words:\n",
    "            if w not in word_to_index:\n",
    "                X_indices[i, j] = 0 # HACK - FIX SOON\n",
    "            else:\n",
    "                if j >= maxLen:\n",
    "                    print (sentence_words)\n",
    "                X_indices[i, j] = word_to_index[w]\n",
    "            j = j+1\n",
    "    return X_indices\n",
    "\n",
    "def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",
    "    \"\"\"\n",
    "    Creates a Keras Embedding() layer and loads in pre-trained GloVe 50-dimensional vectors.\n",
    "    \n",
    "    Arguments:\n",
    "    word_to_vec_map -- dictionary mapping words to their GloVe vector representation.\n",
    "    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)\n",
    "\n",
    "    Returns:\n",
    "    embedding_layer -- pretrained layer Keras instance\n",
    "    \"\"\"\n",
    "\n",
    "    vocab_len = len(word_to_index) + 1\n",
    "    emb_dim = word_to_vec_map[\"lemon\"].shape[0]\n",
    "    emb_matrix = np.zeros((vocab_len, emb_dim)) # curious why not transpose of this...\n",
    "    # Sets each row \"index\" of the embedding matrix to be \n",
    "    # the word vector representation of the \"index\"th word of the vocabulary\n",
    "    for word, index in word_to_index.items():\n",
    "        emb_matrix[index, :] = word_to_vec_map[word]\n",
    "    \n",
    "    embedding_layer = Embedding(vocab_len, emb_dim, trainable = False)\n",
    "\n",
    "    embedding_layer.build((None,))\n",
    "    embedding_layer.set_weights([emb_matrix]) # now it's pretrained!\n",
    "\n",
    "    return embedding_layer\n",
    "\n",
    "embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "X_train_indices = sentences_to_indices(X_train, word_to_index, maxLen)\n",
    "X_test_indices = sentences_to_indices(X_test, word_to_index, max_len = maxLen)\n",
    "X_dev_indices = sentences_to_indices(X_dev, word_to_index, maxLen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Model_V1(input_shape, word_to_vec_map, word_to_index):\n",
    "    \"\"\"\n",
    "    Function creating the Model-V1 model's graph.\n",
    "    \n",
    "    Arguments:\n",
    "    input_shape -- shape of the input, usually (max_len,)\n",
    "    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation\n",
    "    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)\n",
    "\n",
    "    Returns:\n",
    "    model -- a model instance in Keras\n",
    "    \"\"\"\n",
    "    \n",
    "    sentence_indices = Input(shape = input_shape, dtype = np.int32)\n",
    "    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "\n",
    "    # Propagates sentence_indices through the embedding layer\n",
    "    embeddings = embedding_layer(sentence_indices)\n",
    "\n",
    "    # Propagate the embeddings through an LSTM layer with 128-dimensional hidden state\n",
    "    LSTM1 = LSTM(128, return_sequences = True,name='LSTM1')(embeddings)\n",
    "    # Adds dropout with probability 0.5\n",
    "    X = Dropout(0.5)(LSTM1)\n",
    "    # Another LSTM layer, but just returns one output\n",
    "    LSTM2 = LSTM(128, return_sequences = True, name='LSTM2')(X)\n",
    "    \n",
    "    def get_last(X):\n",
    "        return X[:,-1,:]\n",
    "    \n",
    "    LSTM2Last = Lambda(get_last, name='LSTM2-last')(LSTM2)\n",
    "    Dropout2 = Dropout(0.5,name='Dropout2')(LSTM2Last)\n",
    "    \n",
    "    # Propagating through a Dense layer with sigmoid activation to get back a scalar\n",
    "    Dense1 = Dense(1,name='Dense1')(Dropout2)\n",
    "    X = Activation('sigmoid',name='output_layer')(Dense1)\n",
    "\n",
    "    model = Model(inputs = sentence_indices, outputs = X)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adjustable model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Model_V2(input_shape, word_to_vec_map, word_to_index,num_layer,num_cell,dropout_ratio,bidirectional):\n",
    "    \"\"\"\n",
    "    Function creating the Model-V1 model's graph.\n",
    "    \n",
    "    Arguments:\n",
    "    input_shape -- shape of the input, usually (max_len,)\n",
    "    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation\n",
    "    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)\n",
    "\n",
    "    Returns:\n",
    "    model -- a model instance in Keras\n",
    "    \"\"\"\n",
    "    \n",
    "    sentence_indices = Input(shape = input_shape, dtype = np.int32)\n",
    "    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "\n",
    "    # Propagates sentence_indices through the embedding layer\n",
    "    X = embedding_layer(sentence_indices)\n",
    "    \n",
    "    # add the first layer, if there is any.\n",
    "    if num_layer == 2:\n",
    "        print (2)\n",
    "        # Propagate the embeddings through an LSTM layer with 128-dimensional hidden state\n",
    "        if bidirectional == True:\n",
    "            print ('b1')\n",
    "            LSTM1 = Bidirectional(LSTM(num_cell, return_sequences = True),name='LSTM1')(X)\n",
    "        else:\n",
    "            print('l1')\n",
    "            LSTM1 = LSTM(num_cell, return_sequences = True,name='LSTM1')(X)\n",
    "        # Adds dropout with probability 0.5\n",
    "        X = Dropout(dropout_ratio)(LSTM1)\n",
    "\n",
    "    # add second layer (or the only layer)\n",
    "    if  num_layer == 1 and bidirectional == True:\n",
    "        print ('b2')\n",
    "        LSTM2 = Bidirectional(LSTM(num_cell, return_sequences = True), name='LSTM2')(X)\n",
    "    else:\n",
    "        print ('l2')\n",
    "    # Another LSTM layer, but just returns one output\n",
    "        LSTM2 = LSTM(num_cell, return_sequences = True, name='LSTM2')(X)\n",
    "    \n",
    "    def get_last(X):\n",
    "        return X[:,-1,:]\n",
    "    \n",
    "    LSTM2Last = Lambda(get_last, name='LSTM2-last')(LSTM2)\n",
    "    Dropout2 = Dropout(dropout_ratio,name='Dropout2')(LSTM2Last)\n",
    "    \n",
    "    # Propagating through a Dense layer with sigmoid activation to get back a scalar\n",
    "    Dense1 = Dense(1,name='Dense1')(Dropout2)\n",
    "    X = Activation('sigmoid',name='output_layer')(Dense1)\n",
    "\n",
    "    model = Model(inputs = sentence_indices, outputs = X)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model\n",
    "num_layer=[1] # index 0 is better\n",
    "num_cell=[64] # 0 index\n",
    "drop_ratio=[0.3] # 2 index\n",
    "bidirectional=[False] # 0 index\n",
    "\n",
    "# optimizer\n",
    "beta1=0.9\n",
    "beta2=0.999\n",
    "\n",
    "#fitting\n",
    "learning_rate=[0.001]#[0.001,0.002,0.003,0.004]\n",
    "batch_size=[300]#[5,10,15,20,25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l2\n",
      "Train on 9908 samples, validate on 184 samples\n",
      "Epoch 1/20\n",
      "9908/9908 [==============================] - 15s 1ms/step - loss: 0.6911 - acc: 0.5230 - val_loss: 0.7353 - val_acc: 0.3424\n",
      "Epoch 2/20\n",
      "9908/9908 [==============================] - 2s 181us/step - loss: 0.6647 - acc: 0.5897 - val_loss: 0.6621 - val_acc: 0.6576\n",
      "Epoch 3/20\n",
      "9908/9908 [==============================] - 2s 178us/step - loss: 0.6171 - acc: 0.6509 - val_loss: 0.5773 - val_acc: 0.7120\n",
      "Epoch 4/20\n",
      "9908/9908 [==============================] - 2s 181us/step - loss: 0.5846 - acc: 0.6900 - val_loss: 0.6051 - val_acc: 0.6848\n",
      "Epoch 5/20\n",
      "9908/9908 [==============================] - 2s 181us/step - loss: 0.6154 - acc: 0.6655 - val_loss: 0.4851 - val_acc: 0.7609\n",
      "Epoch 6/20\n",
      "9908/9908 [==============================] - 2s 179us/step - loss: 0.5414 - acc: 0.7302 - val_loss: 0.4702 - val_acc: 0.7500\n",
      "Epoch 7/20\n",
      "9908/9908 [==============================] - 2s 179us/step - loss: 0.5202 - acc: 0.7445 - val_loss: 0.5179 - val_acc: 0.7337\n",
      "Epoch 8/20\n",
      "9908/9908 [==============================] - 2s 179us/step - loss: 0.4962 - acc: 0.7627 - val_loss: 0.4320 - val_acc: 0.7717\n",
      "Epoch 9/20\n",
      "9908/9908 [==============================] - 2s 179us/step - loss: 0.4735 - acc: 0.7779 - val_loss: 0.4364 - val_acc: 0.7663\n",
      "Epoch 10/20\n",
      "9908/9908 [==============================] - 2s 183us/step - loss: 0.4422 - acc: 0.7999 - val_loss: 0.4677 - val_acc: 0.7500\n",
      "Epoch 11/20\n",
      "9908/9908 [==============================] - 2s 190us/step - loss: 0.4719 - acc: 0.7783 - val_loss: 0.3903 - val_acc: 0.8043\n",
      "Epoch 12/20\n",
      "9908/9908 [==============================] - 2s 185us/step - loss: 0.4132 - acc: 0.8119 - val_loss: 0.4139 - val_acc: 0.8098\n",
      "Epoch 13/20\n",
      "9908/9908 [==============================] - 2s 186us/step - loss: 0.4117 - acc: 0.8157 - val_loss: 0.3419 - val_acc: 0.8424\n",
      "Epoch 14/20\n",
      "9908/9908 [==============================] - 2s 194us/step - loss: 0.3863 - acc: 0.8299 - val_loss: 0.3758 - val_acc: 0.8261\n",
      "Epoch 15/20\n",
      "9908/9908 [==============================] - 2s 196us/step - loss: 0.3839 - acc: 0.8292 - val_loss: 0.4230 - val_acc: 0.8043\n",
      "Epoch 16/20\n",
      "9908/9908 [==============================] - 2s 189us/step - loss: 0.3922 - acc: 0.8281 - val_loss: 0.3542 - val_acc: 0.8207\n",
      "Epoch 17/20\n",
      "9908/9908 [==============================] - 2s 192us/step - loss: 0.3608 - acc: 0.8428 - val_loss: 0.3763 - val_acc: 0.8370\n",
      "Epoch 18/20\n",
      "9908/9908 [==============================] - 2s 190us/step - loss: 0.3524 - acc: 0.8444 - val_loss: 0.4177 - val_acc: 0.8098\n",
      "Epoch 19/20\n",
      "9908/9908 [==============================] - 2s 191us/step - loss: 0.3300 - acc: 0.8571 - val_loss: 0.3212 - val_acc: 0.8533\n",
      "Epoch 20/20\n",
      "9908/9908 [==============================] - 2s 197us/step - loss: 0.3358 - acc: 0.8573 - val_loss: 0.3235 - val_acc: 0.8696\n",
      "1663/1663 [==============================] - 0s 210us/step\n",
      "Test accuracy =  0.846662657847264\n"
     ]
    }
   ],
   "source": [
    "il=0\n",
    "ic=0\n",
    "idr=0\n",
    "ibr=0\n",
    "# might want to change the metric here\n",
    "model = Model_V2((maxLen,), word_to_vec_map, word_to_index,num_layer[il],num_cell[ic],drop_ratio[idr],bidirectional[ibr])\n",
    "optimizer = Adam(lr=learning_rate[ilr], beta_1=beta1, beta_2=beta2, decay=0.0, epsilon=None)\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "# train the model\n",
    "model_fitting = model.fit(X_train_indices, Y_train, epochs = 20, batch_size = batch_size[ibs], shuffle=True,validation_data=(X_dev_indices, Y_dev))\n",
    "loss, acc = model.evaluate(X_test_indices, Y_test)\n",
    "model.save('word_model_PB.h5')\n",
    "print(\"Test accuracy = \", acc)\n",
    "val_acc[il][ic][idr][ibr][ilr][ibs] = model_fitting.history['val_acc'][-1]\n",
    "tra_acc[il][ic][idr][ibr][ilr][ibs] = model_fitting.history['acc'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8414, 0.9076]"
      ]
     },
     "execution_count": 602,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[0.8414 ,0.9076]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for il in range(len(num_layer)):\n",
    "    for ic in range(len(num_cell)):\n",
    "        for idr in range(len(drop_ratio)):\n",
    "            for ibr in range(len(bidirectional)):\n",
    "                for ilr in range(len(learning_rate)):\n",
    "                    for ibs in range(len(batch_size)):\n",
    "                        print ('num_layer:'+str(num_layer[il]),'num_cell:'+str(num_cell[ic]),'drop_ratio:'+str(drop_ratio[idr]),'bidirectional:'+str(bidirectional[ibr]))\n",
    "                        print('val_acc'+str(val_acc[il][ic][idr][ibr][ilr][ibs]),'tra_acc'+str(tra_acc[il][ic][idr][ibr][ilr][ibs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_acc=np.zeros([len(num_layer),len(num_cell),len(drop_ratio),len(bidirectional),len(learning_rate),len(batch_size)])\n",
    "tra_acc=np.zeros([len(num_layer),len(num_cell),len(drop_ratio),len(bidirectional),len(learning_rate),len(batch_size)])\n",
    "for il in range(len(num_layer)):\n",
    "    for ic in range(len(num_cell)):\n",
    "        for idr in range(len(drop_ratio)):\n",
    "            for ibr in range(len(bidirectional)):\n",
    "                for ilr in range(len(learning_rate)):\n",
    "                    for ibs in range(len(batch_size)):\n",
    "                        # might want to change the metric here\n",
    "                        model = Model_V2((maxLen,), word_to_vec_map, word_to_index,num_layer[il],num_cell[ic],drop_ratio[idr],bidirectional[ibr])\n",
    "                        optimizer = Adam(lr=learning_rate[ilr], beta_1=beta1, beta_2=beta2, decay=0.0, epsilon=None)\n",
    "                        model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "                        # train the model\n",
    "                        model_fitting = model.fit(X_train_indices, Y_train, epochs = 20, batch_size = batch_size[ibs], shuffle=True,validation_data=(X_dev_indices, Y_dev))\n",
    "                        loss, acc = model.evaluate(X_test_indices, Y_test)\n",
    "                        model.save('my_model.h5')\n",
    "                        print(\"Test accuracy = \", acc)\n",
    "                        val_acc[il][ic][idr][ibr][ilr][ibs] = model_fitting.history['val_acc'][-1]\n",
    "                        tra_acc[il][ic][idr][ibr][ilr][ibs] = model_fitting.history['acc'][-1]\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(val_acc.shape)\n",
    "np.mean(val_acc[:,:,:,:,:,:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8572870396200957"
      ]
     },
     "execution_count": 603,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fitting.history['val_acc'][-1]\n",
    "model_fitting.history['acc'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.engine import Input, Model, InputSpec\n",
    "from keras.layers import Dense, Activation, Dropout, Lambda\n",
    "from keras.layers import Embedding, LSTM\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.datasets import imdb\n",
    "from keras.models import load_model\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loading the model\n",
    "from keras.models import load_model\n",
    "model = load_model('word_1lay_64cell_30drop_1dir_60ep_50kex.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_model_bs(model, include_gradients=False):\n",
    "\n",
    "    LSTM2 = model.get_layer('LSTM2')\n",
    "    Dropout2 = model.get_layer('Dropout2')\n",
    "    output_layer = model.get_layer('output_layer')\n",
    "\n",
    "    inputs = []\n",
    "    inputs.extend(model.inputs)\n",
    "\n",
    "    outputs = []\n",
    "    outputs.extend(model.outputs)\n",
    "    outputs.append(LSTM2.output)\n",
    "    outputs.append(LSTM2.cell.kernel_f)  # -- weights of the forget gates (assuming LSTM)\n",
    "    #print (LSTM1.trainable_weights)\n",
    "\n",
    "    if include_gradients:\n",
    "        loss = K.mean(model.output)  # [batch_size, 1] -> scalar\n",
    "        grads = K.gradients(loss, LSTM2.output)\n",
    "        grads_norm = grads / (K.sqrt(K.mean(K.square(grads))) + 1e-5)\n",
    "        outputs.append(grads_norm)\n",
    "\n",
    "    all_function = K.function(inputs, outputs)\n",
    "    output_function = K.function([Dropout2.input], model.outputs)\n",
    "    print(Dropout2.input)\n",
    "    return all_function, output_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Reshape:0\", shape=(1, 1, 64), dtype=float32)\n",
      "Tensor(\"LSTM2_74/transpose_1:0\", shape=(?, ?, 64), dtype=float32)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-608-9bd4ea4c951c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentences_to_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_to_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxLen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mget_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetWeightedOutputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mrnn_values_weighted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/bopeng/Library/TF/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2476\u001b[0m                                           np.expand_dims(sparse_coo.col, 1)), 1)\n\u001b[1;32m   2477\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_coo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_coo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2478\u001b[0;31m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2479\u001b[0m         \u001b[0mfetches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdates_op\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "# Not yet working...\n",
    "import tensorflow as tf\n",
    "\n",
    "def getWeightedOutputs(model):\n",
    "\n",
    "    LSTM2 = model.get_layer('LSTM2')\n",
    "    Dense1 = model.get_layer('Dense1')\n",
    "    \n",
    "    weights = tf.convert_to_tensor(Dense1.get_weights()[0], dtype=np.float32)\n",
    "    weights = tf.reshape(weights,(1,1,64))\n",
    "    print(weights)\n",
    "    print(LSTM2.output)\n",
    "\n",
    "    weightedOutputs = tf.multiply(LSTM2.output, weights)\n",
    "    return K.function([model.inputs], [weightedOutputs])\n",
    "\n",
    "X = sentences_to_indices(t, word_to_index, maxLen)\n",
    "get_weights = getWeightedOutputs(model)\n",
    "rnn_values_weighted = get_weights([X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=load_model('word_1lay_64cell_30drop_1dir_60ep_50kex.h5')\n",
    "all_function, output_function = visualize_model_bs(model, include_gradients=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1) (1, 33, 64) (50, 64) (1, 1, 33, 64)\n",
      "Scores: [[0.00015642]]\n",
      "Time distributed (word-level) scores: [array([[5.5311471e-01],\n",
      "       [7.5159383e-01],\n",
      "       [7.9312688e-01],\n",
      "       [6.4650297e-01],\n",
      "       [6.0913348e-01],\n",
      "       [4.6933240e-01],\n",
      "       [5.6290430e-01],\n",
      "       [6.3932526e-01],\n",
      "       [7.3224401e-01],\n",
      "       [8.1640381e-01],\n",
      "       [8.8241988e-01],\n",
      "       [9.2631572e-01],\n",
      "       [9.5257306e-01],\n",
      "       [9.6745300e-01],\n",
      "       [9.7554922e-01],\n",
      "       [9.7955281e-01],\n",
      "       [9.8074728e-01],\n",
      "       [9.7934705e-01],\n",
      "       [9.7429252e-01],\n",
      "       [9.6170306e-01],\n",
      "       [9.2857486e-01],\n",
      "       [8.2825148e-01],\n",
      "       [5.3971142e-01],\n",
      "       [1.5634350e-01],\n",
      "       [2.2301400e-02],\n",
      "       [3.2153302e-03],\n",
      "       [7.9706294e-04],\n",
      "       [3.5778593e-04],\n",
      "       [2.2610926e-04],\n",
      "       [1.7009153e-04],\n",
      "       [1.4622005e-04],\n",
      "       [1.4235283e-04],\n",
      "       [1.5641832e-04]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "maxLen=33\n",
    "sentence=\"clock picking a way him to\"\n",
    "t = np.array([sentence])\n",
    "X = sentences_to_indices(t, word_to_index, maxLen)\n",
    "# -- Return scores, raw rnn values and gradients\n",
    "# scores is equivalent to model.predict(X)\n",
    "scores, rnn_values, rnn_gradients, W_i = all_function([X])\n",
    "print(scores.shape, rnn_values.shape, rnn_gradients.shape, W_i.shape)\n",
    "\n",
    "# -- score prediction\n",
    "print(\"Scores:\", scores)\n",
    "\n",
    "# -- Return scores at each step in the time sequence\n",
    "time_distributed_scores = map(lambda x: output_function([x]), rnn_values)\n",
    "print(\"Time distributed (word-level) scores:\", map(lambda x: x[0], time_distributed_scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    }
   ],
   "source": [
    "print(maxLen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[48;5;0m0\u001b[48;5;1m1\u001b[48;5;2m2\u001b[48;5;3m3\u001b[48;5;4m4\u001b[48;5;5m5\u001b[48;5;6m6\u001b[48;5;7m7\u001b[48;5;8m8\u001b[48;5;9m9\u001b[48;5;10m10\u001b[48;5;11m11\u001b[48;5;12m12\u001b[48;5;13m13\u001b[48;5;14m14\u001b[48;5;15m15\u001b[48;5;16m16\u001b[48;5;17m17\u001b[48;5;18m18\u001b[48;5;19m19\u001b[48;5;20m20\u001b[48;5;21m21\u001b[48;5;22m22\u001b[48;5;23m23\u001b[48;5;24m24\u001b[48;5;25m25\u001b[48;5;26m26\u001b[48;5;27m27\u001b[48;5;28m28\u001b[48;5;29m29\u001b[48;5;30m30\u001b[48;5;31m31\u001b[48;5;32m32\u001b[48;5;33m33\u001b[48;5;34m34\u001b[48;5;35m35\u001b[48;5;36m36\u001b[48;5;37m37\u001b[48;5;38m38\u001b[48;5;39m39\u001b[48;5;40m40\u001b[48;5;41m41\u001b[48;5;42m42\u001b[48;5;43m43\u001b[48;5;44m44\u001b[48;5;45m45\u001b[48;5;46m46\u001b[48;5;47m47\u001b[48;5;48m48\u001b[48;5;49m49\u001b[48;5;50m50\u001b[48;5;51m51\u001b[48;5;52m52\u001b[48;5;53m53\u001b[48;5;54m54\u001b[48;5;55m55\u001b[48;5;56m56\u001b[48;5;57m57\u001b[48;5;58m58\u001b[48;5;59m59\u001b[48;5;60m60\u001b[48;5;61m61\u001b[48;5;62m62\u001b[48;5;63m63\u001b[48;5;64m64\u001b[48;5;65m65\u001b[48;5;66m66\u001b[48;5;67m67\u001b[48;5;68m68\u001b[48;5;69m69\u001b[48;5;70m70\u001b[48;5;71m71\u001b[48;5;72m72\u001b[48;5;73m73\u001b[48;5;74m74\u001b[48;5;75m75\u001b[48;5;76m76\u001b[48;5;77m77\u001b[48;5;78m78\u001b[48;5;79m79\u001b[48;5;80m80\u001b[48;5;81m81\u001b[48;5;82m82\u001b[48;5;83m83\u001b[48;5;84m84\u001b[48;5;85m85\u001b[48;5;86m86\u001b[48;5;87m87\u001b[48;5;88m88\u001b[48;5;89m89\u001b[48;5;90m90\u001b[48;5;91m91\u001b[48;5;92m92\u001b[48;5;93m93\u001b[48;5;94m94\u001b[48;5;95m95\u001b[48;5;96m96\u001b[48;5;97m97\u001b[48;5;98m98\u001b[48;5;99m99\u001b[48;5;100m100\u001b[48;5;101m101\u001b[48;5;102m102\u001b[48;5;103m103\u001b[48;5;104m104\u001b[48;5;105m105\u001b[48;5;106m106\u001b[48;5;107m107\u001b[48;5;108m108\u001b[48;5;109m109\u001b[48;5;110m110\u001b[48;5;111m111\u001b[48;5;112m112\u001b[48;5;113m113\u001b[48;5;114m114\u001b[48;5;115m115\u001b[48;5;116m116\u001b[48;5;117m117\u001b[48;5;118m118\u001b[48;5;119m119\u001b[48;5;120m120\u001b[48;5;121m121\u001b[48;5;122m122\u001b[48;5;123m123\u001b[48;5;124m124\u001b[48;5;125m125\u001b[48;5;126m126\u001b[48;5;127m127\u001b[48;5;128m128\u001b[48;5;129m129\u001b[48;5;130m130\u001b[48;5;131m131\u001b[48;5;132m132\u001b[48;5;133m133\u001b[48;5;134m134\u001b[48;5;135m135\u001b[48;5;136m136\u001b[48;5;137m137\u001b[48;5;138m138\u001b[48;5;139m139\u001b[48;5;140m140\u001b[48;5;141m141\u001b[48;5;142m142\u001b[48;5;143m143\u001b[48;5;144m144\u001b[48;5;145m145\u001b[48;5;146m146\u001b[48;5;147m147\u001b[48;5;148m148\u001b[48;5;149m149\u001b[48;5;150m150\u001b[48;5;151m151\u001b[48;5;152m152\u001b[48;5;153m153\u001b[48;5;154m154\u001b[48;5;155m155\u001b[48;5;156m156\u001b[48;5;157m157\u001b[48;5;158m158\u001b[48;5;159m159\u001b[48;5;160m160\u001b[48;5;161m161\u001b[48;5;162m162\u001b[48;5;163m163\u001b[48;5;164m164\u001b[48;5;165m165\u001b[48;5;166m166\u001b[48;5;167m167\u001b[48;5;168m168\u001b[48;5;169m169\u001b[48;5;170m170\u001b[48;5;171m171\u001b[48;5;172m172\u001b[48;5;173m173\u001b[48;5;174m174\u001b[48;5;175m175\u001b[48;5;176m176\u001b[48;5;177m177\u001b[48;5;178m178\u001b[48;5;179m179\u001b[48;5;180m180\u001b[48;5;181m181\u001b[48;5;182m182\u001b[48;5;183m183\u001b[48;5;184m184\u001b[48;5;185m185\u001b[48;5;186m186\u001b[48;5;187m187\u001b[48;5;188m188\u001b[48;5;189m189\u001b[48;5;190m190\u001b[48;5;191m191\u001b[48;5;192m192\u001b[48;5;193m193\u001b[48;5;194m194\u001b[48;5;195m195\u001b[48;5;196m196\u001b[48;5;197m197\u001b[48;5;198m198\u001b[48;5;199m199\u001b[48;5;200m200\u001b[48;5;201m201\u001b[48;5;202m202\u001b[48;5;203m203\u001b[48;5;204m204\u001b[48;5;205m205\u001b[48;5;206m206\u001b[48;5;207m207\u001b[48;5;208m208\u001b[48;5;209m209\u001b[48;5;210m210\u001b[48;5;211m211\u001b[48;5;212m212\u001b[48;5;213m213\u001b[48;5;214m214\u001b[48;5;215m215\u001b[48;5;216m216\u001b[48;5;217m217\u001b[48;5;218m218\u001b[48;5;219m219\u001b[48;5;220m220\u001b[48;5;221m221\u001b[48;5;222m222\u001b[48;5;223m223\u001b[48;5;224m224\u001b[48;5;225m225\u001b[48;5;226m226\u001b[48;5;227m227\u001b[48;5;228m228\u001b[48;5;229m229\u001b[48;5;230m230\u001b[48;5;231m231\u001b[48;5;232m232\u001b[48;5;233m233\u001b[48;5;234m234\u001b[48;5;235m235\u001b[48;5;236m236\u001b[48;5;237m237\u001b[48;5;238m238\u001b[48;5;239m239\u001b[48;5;240m240\u001b[48;5;241m241\u001b[48;5;242m242\u001b[48;5;243m243\u001b[48;5;244m244\u001b[48;5;245m245\u001b[48;5;246m246\u001b[48;5;247m247\u001b[48;5;248m248\u001b[48;5;249m249\u001b[48;5;250m250\u001b[48;5;251m251\u001b[48;5;252m252\u001b[48;5;253m253\u001b[48;5;254m254\n",
      "------Color index:-----\n",
      "\u001b[48;5;196m196\u001b[48;5;197m197\u001b[48;5;198m198\u001b[48;5;205m205\u001b[48;5;212m212\u001b[48;5;219m219\u001b[48;5;225m225\u001b[48;5;231m231\u001b[48;5;229m229\u001b[48;5;193m193\u001b[48;5;192m192\u001b[48;5;191m191\u001b[48;5;155m155\u001b[48;5;154m154\u001b[48;5;82m82"
     ]
    }
   ],
   "source": [
    "# choose the color index\n",
    "from colored import fg, bg, attr\n",
    "min_c=0\n",
    "max_c=255\n",
    "for i in range(min_c,max_c):\n",
    "    print ('%s%s' % (bg (i),str(i)),end=\"\")\n",
    "    \n",
    "print('\\n------Color index:-----')\n",
    "\n",
    "color_index=[196,197,198,205,212,219,225,231,229,193,192,191,155,154,82]\n",
    "for i in color_index:\n",
    "    print ('%s%s' % (bg (i),str(i)),end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 9\n",
      "\n",
      "\u001b[48;5;231mNeuron 0: \u001b[48;5;219mclock \u001b[48;5;219mpicking \u001b[48;5;219ma \u001b[48;5;212mway \u001b[48;5;212mhim \u001b[48;5;219mto \n",
      "\u001b[48;5;231mNeuron 1: \u001b[48;5;205mclock \u001b[48;5;205mpicking \u001b[48;5;205ma \u001b[48;5;205mway \u001b[48;5;212mhim \u001b[48;5;225mto \n",
      "\u001b[48;5;231mNeuron 2: \u001b[48;5;205mclock \u001b[48;5;212mpicking \u001b[48;5;212ma \u001b[48;5;212mway \u001b[48;5;219mhim \u001b[48;5;219mto \n",
      "\u001b[48;5;231mNeuron 3: \u001b[48;5;205mclock \u001b[48;5;198mpicking \u001b[48;5;205ma \u001b[48;5;198mway \u001b[48;5;198mhim \u001b[48;5;205mto \n",
      "\u001b[48;5;231mNeuron 4: \u001b[48;5;205mclock \u001b[48;5;198mpicking \u001b[48;5;197ma \u001b[48;5;196mway \u001b[48;5;196mhim \u001b[48;5;197mto \n",
      "\u001b[48;5;231mNeuron 5: \u001b[48;5;219mclock \u001b[48;5;212mpicking \u001b[48;5;212ma \u001b[48;5;205mway \u001b[48;5;212mhim \u001b[48;5;225mto \n",
      "\u001b[48;5;231mNeuron 6: \u001b[48;5;219mclock \u001b[48;5;225mpicking \u001b[48;5;231ma \u001b[48;5;225mway \u001b[48;5;225mhim \u001b[48;5;225mto \n",
      "\u001b[48;5;231mNeuron 7: \u001b[48;5;212mclock \u001b[48;5;212mpicking \u001b[48;5;219ma \u001b[48;5;212mway \u001b[48;5;205mhim \u001b[48;5;205mto \n",
      "\u001b[48;5;231mNeuron 8: \u001b[48;5;219mclock \u001b[48;5;219mpicking \u001b[48;5;219ma \u001b[48;5;219mway \u001b[48;5;219mhim \u001b[48;5;219mto \n",
      "\u001b[48;5;231mNeuron 9: \u001b[48;5;212mclock \u001b[48;5;212mpicking \u001b[48;5;212ma \u001b[48;5;205mway \u001b[48;5;212mhim \u001b[48;5;212mto \n",
      "\u001b[48;5;231mNeuron 10: \u001b[48;5;205mclock \u001b[48;5;212mpicking \u001b[48;5;219ma \u001b[48;5;225mway \u001b[48;5;229mhim \u001b[48;5;225mto \n",
      "\u001b[48;5;231mNeuron 11: \u001b[48;5;212mclock \u001b[48;5;212mpicking \u001b[48;5;219ma \u001b[48;5;212mway \u001b[48;5;212mhim \u001b[48;5;212mto \n",
      "\u001b[48;5;231mNeuron 12: \u001b[48;5;219mclock \u001b[48;5;231mpicking \u001b[48;5;225ma \u001b[48;5;231mway \u001b[48;5;229mhim \u001b[48;5;225mto \n",
      "\u001b[48;5;231mNeuron 13: \u001b[48;5;212mclock \u001b[48;5;205mpicking \u001b[48;5;212ma \u001b[48;5;225mway \u001b[48;5;225mhim \u001b[48;5;225mto \n",
      "\u001b[48;5;231mNeuron 14: \u001b[48;5;225mclock \u001b[48;5;219mpicking \u001b[48;5;219ma \u001b[48;5;219mway \u001b[48;5;219mhim \u001b[48;5;219mto \n",
      "\u001b[48;5;231mNeuron 15: \u001b[48;5;205mclock \u001b[48;5;205mpicking \u001b[48;5;198ma \u001b[48;5;219mway \u001b[48;5;225mhim \u001b[48;5;212mto \n",
      "\u001b[48;5;231mNeuron 16: \u001b[48;5;212mclock \u001b[48;5;212mpicking \u001b[48;5;212ma \u001b[48;5;212mway \u001b[48;5;212mhim \u001b[48;5;205mto \n",
      "\u001b[48;5;231mNeuron 17: \u001b[48;5;219mclock \u001b[48;5;219mpicking \u001b[48;5;219ma \u001b[48;5;219mway \u001b[48;5;212mhim \u001b[48;5;219mto \n",
      "\u001b[48;5;231mNeuron 18: \u001b[48;5;212mclock \u001b[48;5;212mpicking \u001b[48;5;212ma \u001b[48;5;205mway \u001b[48;5;205mhim \u001b[48;5;212mto \n",
      "\u001b[48;5;231mNeuron 19: \u001b[48;5;225mclock \u001b[48;5;219mpicking \u001b[48;5;225ma \u001b[48;5;231mway \u001b[48;5;229mhim \u001b[48;5;225mto \n",
      "\u001b[48;5;231mNeuron 20: \u001b[48;5;212mclock \u001b[48;5;212mpicking \u001b[48;5;219ma \u001b[48;5;219mway \u001b[48;5;212mhim \u001b[48;5;205mto \n",
      "\u001b[48;5;231mNeuron 21: \u001b[48;5;212mclock \u001b[48;5;212mpicking \u001b[48;5;212ma \u001b[48;5;212mway \u001b[48;5;212mhim \u001b[48;5;212mto \n",
      "\u001b[48;5;231mNeuron 22: \u001b[48;5;212mclock \u001b[48;5;225mpicking \u001b[48;5;225ma \u001b[48;5;231mway \u001b[48;5;212mhim \u001b[48;5;198mto \n",
      "\u001b[48;5;231mNeuron 23: \u001b[48;5;225mclock \u001b[48;5;212mpicking \u001b[48;5;229ma \u001b[48;5;198mway \u001b[48;5;205mhim \u001b[48;5;231mto \n",
      "\u001b[48;5;231mNeuron 24: \u001b[48;5;212mclock \u001b[48;5;205mpicking \u001b[48;5;212ma \u001b[48;5;198mway \u001b[48;5;205mhim \u001b[48;5;198mto \n",
      "\u001b[48;5;231mNeuron 25: \u001b[48;5;231mclock \u001b[48;5;225mpicking \u001b[48;5;231ma \u001b[48;5;225mway \u001b[48;5;225mhim \u001b[48;5;219mto \n",
      "\u001b[48;5;231mNeuron 26: \u001b[48;5;205mclock \u001b[48;5;205mpicking \u001b[48;5;198ma \u001b[48;5;205mway \u001b[48;5;198mhim \u001b[48;5;197mto \n",
      "\u001b[48;5;231mNeuron 27: \u001b[48;5;212mclock \u001b[48;5;219mpicking \u001b[48;5;219ma \u001b[48;5;219mway \u001b[48;5;219mhim \u001b[48;5;212mto \n",
      "\u001b[48;5;231mNeuron 28: \u001b[48;5;212mclock \u001b[48;5;219mpicking \u001b[48;5;229ma \u001b[48;5;231mway \u001b[48;5;231mhim \u001b[48;5;193mto \n",
      "\u001b[48;5;231mNeuron 29: \u001b[48;5;219mclock \u001b[48;5;212mpicking \u001b[48;5;198ma \u001b[48;5;197mway \u001b[48;5;219mhim \u001b[48;5;212mto \n",
      "\u001b[48;5;231mNeuron 30: \u001b[48;5;225mclock \u001b[48;5;219mpicking \u001b[48;5;219ma \u001b[48;5;219mway \u001b[48;5;205mhim \u001b[48;5;205mto \n",
      "\u001b[48;5;231mNeuron 31: \u001b[48;5;212mclock \u001b[48;5;231mpicking \u001b[48;5;197ma \u001b[48;5;212mway \u001b[48;5;225mhim \u001b[48;5;231mto \n",
      "\u001b[48;5;231mNeuron 32: \u001b[48;5;212mclock \u001b[48;5;212mpicking \u001b[48;5;212ma \u001b[48;5;212mway \u001b[48;5;212mhim \u001b[48;5;212mto \n",
      "\u001b[48;5;231mNeuron 33: \u001b[48;5;231mclock \u001b[48;5;219mpicking \u001b[48;5;219ma \u001b[48;5;212mway \u001b[48;5;219mhim \u001b[48;5;212mto \n",
      "\u001b[48;5;231mNeuron 34: \u001b[48;5;212mclock \u001b[48;5;205mpicking \u001b[48;5;212ma \u001b[48;5;225mway \u001b[48;5;212mhim \u001b[48;5;205mto \n",
      "\u001b[48;5;231mNeuron 35: \u001b[48;5;212mclock \u001b[48;5;212mpicking \u001b[48;5;205ma \u001b[48;5;205mway \u001b[48;5;219mhim \u001b[48;5;219mto \n",
      "\u001b[48;5;231mNeuron 36: \u001b[48;5;219mclock \u001b[48;5;225mpicking \u001b[48;5;225ma \u001b[48;5;225mway \u001b[48;5;225mhim \u001b[48;5;219mto \n",
      "\u001b[48;5;231mNeuron 37: \u001b[48;5;212mclock \u001b[48;5;212mpicking \u001b[48;5;212ma \u001b[48;5;205mway \u001b[48;5;205mhim \u001b[48;5;212mto \n",
      "\u001b[48;5;231mNeuron 38: \u001b[48;5;212mclock \u001b[48;5;205mpicking \u001b[48;5;205ma \u001b[48;5;212mway \u001b[48;5;205mhim \u001b[48;5;205mto \n",
      "\u001b[48;5;231mNeuron 39: \u001b[48;5;212mclock \u001b[48;5;205mpicking \u001b[48;5;205ma \u001b[48;5;197mway \u001b[48;5;197mhim \u001b[48;5;197mto \n",
      "\u001b[48;5;231mNeuron 40: \u001b[48;5;212mclock \u001b[48;5;212mpicking \u001b[48;5;219ma \u001b[48;5;219mway \u001b[48;5;205mhim \u001b[48;5;212mto \n",
      "\u001b[48;5;231mNeuron 41: \u001b[48;5;219mclock \u001b[48;5;219mpicking \u001b[48;5;212ma \u001b[48;5;219mway \u001b[48;5;231mhim \u001b[48;5;198mto \n",
      "\u001b[48;5;231mNeuron 42: \u001b[48;5;225mclock \u001b[48;5;225mpicking \u001b[48;5;229ma \u001b[48;5;212mway \u001b[48;5;219mhim \u001b[48;5;219mto \n",
      "\u001b[48;5;231mNeuron 43: \u001b[48;5;212mclock \u001b[48;5;219mpicking \u001b[48;5;225ma \u001b[48;5;231mway \u001b[48;5;219mhim \u001b[48;5;225mto \n",
      "\u001b[48;5;231mNeuron 44: \u001b[48;5;225mclock \u001b[48;5;205mpicking \u001b[48;5;212ma \u001b[48;5;219mway \u001b[48;5;225mhim \u001b[48;5;219mto \n",
      "\u001b[48;5;231mNeuron 45: \u001b[48;5;219mclock \u001b[48;5;219mpicking \u001b[48;5;225ma \u001b[48;5;219mway \u001b[48;5;231mhim \u001b[48;5;225mto \n",
      "\u001b[48;5;231mNeuron 46: \u001b[48;5;212mclock \u001b[48;5;198mpicking \u001b[48;5;205ma \u001b[48;5;198mway \u001b[48;5;205mhim \u001b[48;5;212mto \n",
      "\u001b[48;5;231mNeuron 47: \u001b[48;5;212mclock \u001b[48;5;212mpicking \u001b[48;5;205ma \u001b[48;5;198mway \u001b[48;5;205mhim \u001b[48;5;198mto \n",
      "\u001b[48;5;231mNeuron 48: \u001b[48;5;205mclock \u001b[48;5;212mpicking \u001b[48;5;212ma \u001b[48;5;225mway \u001b[48;5;212mhim \u001b[48;5;219mto \n",
      "\u001b[48;5;231mNeuron 49: \u001b[48;5;225mclock \u001b[48;5;225mpicking \u001b[48;5;225ma \u001b[48;5;231mway \u001b[48;5;225mhim \u001b[48;5;225mto \n",
      "\u001b[48;5;231mNeuron 50: \u001b[48;5;219mclock \u001b[48;5;219mpicking \u001b[48;5;225ma \u001b[48;5;225mway \u001b[48;5;225mhim \u001b[48;5;225mto \n",
      "\u001b[48;5;231mNeuron 51: \u001b[48;5;225mclock \u001b[48;5;231mpicking \u001b[48;5;193ma \u001b[48;5;225mway \u001b[48;5;225mhim \u001b[48;5;219mto \n",
      "\u001b[48;5;231mNeuron 52: \u001b[48;5;219mclock \u001b[48;5;225mpicking \u001b[48;5;231ma \u001b[48;5;229mway \u001b[48;5;231mhim \u001b[48;5;225mto \n",
      "\u001b[48;5;231mNeuron 53: \u001b[48;5;212mclock \u001b[48;5;205mpicking \u001b[48;5;212ma \u001b[48;5;212mway \u001b[48;5;205mhim \u001b[48;5;212mto \n",
      "\u001b[48;5;231mNeuron 54: \u001b[48;5;212mclock \u001b[48;5;198mpicking \u001b[48;5;197ma \u001b[48;5;198mway \u001b[48;5;198mhim \u001b[48;5;212mto \n",
      "\u001b[48;5;231mNeuron 55: \u001b[48;5;212mclock \u001b[48;5;212mpicking \u001b[48;5;212ma \u001b[48;5;212mway \u001b[48;5;205mhim \u001b[48;5;212mto \n",
      "\u001b[48;5;231mNeuron 56: \u001b[48;5;219mclock \u001b[48;5;212mpicking \u001b[48;5;212ma \u001b[48;5;212mway \u001b[48;5;205mhim \u001b[48;5;205mto \n",
      "\u001b[48;5;231mNeuron 57: \u001b[48;5;219mclock \u001b[48;5;219mpicking \u001b[48;5;231ma \u001b[48;5;231mway \u001b[48;5;229mhim \u001b[48;5;193mto \n",
      "\u001b[48;5;231mNeuron 58: \u001b[48;5;205mclock \u001b[48;5;198mpicking \u001b[48;5;212ma \u001b[48;5;212mway \u001b[48;5;212mhim \u001b[48;5;212mto \n",
      "\u001b[48;5;231mNeuron 59: \u001b[48;5;231mclock \u001b[48;5;225mpicking \u001b[48;5;225ma \u001b[48;5;205mway \u001b[48;5;198mhim \u001b[48;5;197mto \n",
      "\u001b[48;5;231mNeuron 60: \u001b[48;5;212mclock \u001b[48;5;205mpicking \u001b[48;5;205ma \u001b[48;5;198mway \u001b[48;5;198mhim \u001b[48;5;198mto \n",
      "\u001b[48;5;231mNeuron 61: \u001b[48;5;225mclock \u001b[48;5;225mpicking \u001b[48;5;225ma \u001b[48;5;212mway \u001b[48;5;205mhim \u001b[48;5;198mto \n",
      "\u001b[48;5;231mNeuron 62: \u001b[48;5;197mclock \u001b[48;5;197mpicking \u001b[48;5;225ma \u001b[48;5;225mway \u001b[48;5;225mhim \u001b[48;5;225mto \n",
      "\u001b[48;5;231mNeuron 63: \u001b[48;5;212mclock \u001b[48;5;212mpicking \u001b[48;5;219ma \u001b[48;5;212mway \u001b[48;5;219mhim \u001b[48;5;219mto \n"
     ]
    }
   ],
   "source": [
    "from colored import fg, bg, attr\n",
    "words=sentence.split()\n",
    "\n",
    "rnn_shape=[1, maxLen, 64]\n",
    "color_index=[196,197,198,205,212,219,225,231,229,193,192,191,155,154,82]\n",
    "len_color=len(color_index)\n",
    "color_weight=np.array(rnn_values)\n",
    "color_weight_range=(max(color_weight.flatten())-min(color_weight.flatten()))+1 # +1 make it safer, and not over the range\n",
    "color_weight=len_color*(color_weight-min(color_weight.flatten()))/color_weight_range\n",
    "color_weight=color_weight.reshape(rnn_shape)\n",
    "\n",
    "\n",
    "# color tuning\n",
    "min_c=int(min(color_weight.flatten()))\n",
    "max_c=int(max(color_weight.flatten()))\n",
    "print(min_c,max_c)\n",
    "\n",
    "    \n",
    "print(\"\")\n",
    "\n",
    "def clamp(x, minval, maxval):\n",
    "    return max(minval, min(x, maxval))\n",
    "\n",
    "\n",
    "for i in range(rnn_shape[2]):\n",
    "    #for w in range(rnn_shape[2]):\n",
    "    print ('%s%s: ' % (bg (231),'Neuron '+str(i)), end=\"\")\n",
    "    for w in range(len(words)):\n",
    "        idx = clamp(int(color_weight[0,w,i]), 0, len(color_index)-1)\n",
    "        print ('%s%s' % (bg (color_index[idx]),words[w]),end=\"\")\n",
    "        print (' ' ,end=\"\")\n",
    "    print (\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.1879048e+00 5.6910758e+00 6.0056238e+00 4.8951759e+00 4.6121602e+00\n",
      " 3.5533843e+00 4.2620459e+00 4.8408160e+00 5.5445313e+00 6.1819105e+00\n",
      " 6.6818795e+00 7.0143223e+00 7.2131805e+00 7.3258729e+00 7.3871894e+00\n",
      " 7.4175105e+00 7.4265566e+00 7.4159517e+00 7.3776722e+00 7.2823257e+00\n",
      " 7.0314317e+00 6.2716384e+00 4.0863962e+00 1.1829807e+00 1.6782030e-01\n",
      " 2.3273021e-02 4.9584103e-03 1.6315705e-03 6.3432462e-04 2.1007750e-04\n",
      " 2.9288136e-05 0.0000000e+00 1.0652419e-04]\n",
      "\u001b[48;5;212mclock\u001b[1m \u001b[48;5;219mpicking\u001b[1m \u001b[48;5;225ma\u001b[1m \u001b[48;5;212mway\u001b[1m \u001b[48;5;212mhim\u001b[1m \u001b[48;5;205mto\u001b[1m "
     ]
    }
   ],
   "source": [
    "from colored import fg, bg, attr\n",
    "\n",
    "words=sentence.split()\n",
    "color_weight=np.array(time_distributed_scores).reshape(maxLen)\n",
    "color_weight_range=max(color_weight)-min(color_weight)+1\n",
    "color_weight=len_color*(color_weight-min(color_weight.flatten()))/color_weight_range\n",
    "\n",
    "print (color_weight)\n",
    "for i in range(len(words)):\n",
    "    print ('%s%s' % (bg (color_index[int(color_weight[i])]),words[i]),end=\"\")\n",
    "    print ('%s '%attr(1) ,end=\"\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9908, 1) (9908, 35, 64) (50, 64) (1, 9908, 35, 64)\n",
      "[[1746  169 2032 ... 9738  432 8956]\n",
      " [1115  665 8865 ... 2186 3956 1668]\n",
      " [8274 1403 3507 ... 4006   96  426]\n",
      " ...\n",
      " [8837 2971 2266 ... 6348 4645 1714]\n",
      " [   0    0    0 ...    0    0    0]\n",
      " [   0    0    0 ...    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "maxLen=33\n",
    "\n",
    "\n",
    "# -- Return scores, raw rnn values and gradients\n",
    "# scores is equivalent to model.predict(X)\n",
    "scores, rnn_values, rnn_gradients, W_i = all_function([X_train_indices])\n",
    "print(scores.shape, rnn_values.shape, rnn_gradients.shape, W_i.shape)\n",
    "\n",
    "maxes = tf.Session().run(tf.argmax(rnn_values, axis=0))\n",
    "print(maxes)\n",
    "\n",
    "# -- Return scores at each step in the time sequence\n",
    "# time_distributed_scores = map(lambda x: output_function([x]), rnn_values)\n",
    "# print(\"Time distributed (word-level) scores:\", map(lambda x: x[0], time_distributed_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8837 2971 2266  920 1942 2503 7300 5181  436 7696 5313 4328 5896 4645\n",
      " 3716 3920 8956 7124 4690 9465 3563 2909 5181 4758  573 4313 4690 1942\n",
      " 4328 3263 7699 4758  573 4601 4153 4690 5415 4082 5899 2909 5687 1714\n",
      " 2114 6577  170 4690 7746  608 7309 9507 1995 9536   96 5555 6577 7699\n",
      " 3263 5181 4153 4548 4314 6348 4645 1714]\n"
     ]
    }
   ],
   "source": [
    "lastSlice = rnn_values[:,-3,:] # all zeroes if it's -2 etc\n",
    "lastMaxes = tf.Session().run(tf.argmax(lastSlice, axis=0))\n",
    "print(lastMaxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['In the past I used to vote the Democratic ticket', 'in The I the',\n",
       "       \"They say fine words are no virtue if they're insincere and that's him in a nutshell He's all talk but doesn't mean a word of it\",\n",
       "       'No',\n",
       "       '26511\\tfra\\tAimes-tu les serpents\\xe2\\x80\\xaf Bien s\\xc3\\xbbr que non',\n",
       "       'this hours the', 'down is taken trees eggs with they',\n",
       "       \"The information presented in Kelly's paper on color coordination is seen to be of use in building up an alternative theory\",\n",
       "       \"soccer Some doubtful people children don't reminds be at ish wish\",\n",
       "       'star weight prove will before business to a excellent out trumpet',\n",
       "       'almost past people good property may too teacher dress make a',\n",
       "       'One of the reasons is the difference between Japan and other countries in their attitudes toward education',\n",
       "       \"day This a read the in hear her hiked I'm payment latest\",\n",
       "       'We accept making a reduction in price of 5% if this will help you to develop a new market for our products',\n",
       "       'The gate is closed all the year round',\n",
       "       'lost to too about Bark to how doubts the an than here',\n",
       "       'Soviet and Western observers have warned that if the Muslim republics do not join the commonwealth',\n",
       "       'two well late the together homework Liquids school The',\n",
       "       \"Credit is the amount or sum placed at a person's disposal by a bank; a loan of money\",\n",
       "       'The fishermen cast their nets into the sea',\n",
       "       'said Americans has ideas We mystery who',\n",
       "       'model solved conference explain Much to accord of',\n",
       "       \"The information presented in Kelly's paper on color coordination is seen to be of use in building up an alternative theory\",\n",
       "       'Is it possible to reproduce 70 copies of your report which appeared in the November issue of The Network and distribute them to our agents',\n",
       "       'The company provides health care and life insurance benefits for all of its employees',\n",
       "       'The farm grows potatoes',\n",
       "       \"Credit is the amount or sum placed at a person's disposal by a bank; a loan of money\",\n",
       "       '26511\\tfra\\tAimes-tu les serpents\\xe2\\x80\\xaf Bien s\\xc3\\xbbr que non',\n",
       "       'One of the reasons is the difference between Japan and other countries in their attitudes toward education',\n",
       "       'There the spider waits for small insects like butterflies and dragonflies to be trapped',\n",
       "       'I am wondering if you would like to go and see Kabuki with me while staying in Japan',\n",
       "       'Is it possible to reproduce 70 copies of your report which appeared in the November issue of The Network and distribute them to our agents',\n",
       "       'The company provides health care and life insurance benefits for all of its employees',\n",
       "       \"to of The Painted We the You'll I'm\",\n",
       "       \"is before speak reform will don't\",\n",
       "       \"Credit is the amount or sum placed at a person's disposal by a bank; a loan of money\",\n",
       "       'The news that her son was injured in the accident was a great shock to her',\n",
       "       'of justice of here at is one you task help to',\n",
       "       'they our asked Wake speak mountain arrived a heartily in congratulate belt',\n",
       "       'model solved conference explain Much to accord of',\n",
       "       'foot necessary rights view difference having necessary of my serious will ish',\n",
       "       'The habits of highly intelligent people offer a clue as to how to do that',\n",
       "       \"I gotta keep on movin'\", 'Please Oww is', 'the You the with the',\n",
       "       \"Credit is the amount or sum placed at a person's disposal by a bank; a loan of money\",\n",
       "       'fishing it What took and miles be There news Food 20',\n",
       "       'Past the west coasts of Europe and Africa to the tip of southern Africa',\n",
       "       'To begin with', 'Toil and worry caused his health to break down',\n",
       "       'The robbers made away with all the money in the safe',\n",
       "       \"Susan shined her father's shoes\",\n",
       "       'to Hawaii facing has bigger innocence sure the come',\n",
       "       'I has ultimate to always', 'Please Oww is',\n",
       "       'I am wondering if you would like to go and see Kabuki with me while staying in Japan',\n",
       "       'There the spider waits for small insects like butterflies and dragonflies to be trapped',\n",
       "       \"The information presented in Kelly's paper on color coordination is seen to be of use in building up an alternative theory\",\n",
       "       \"is before speak reform will don't\",\n",
       "       'Our plane was flying over the Pacific Ocean',\n",
       "       \"chapter answers a I your I'm\",\n",
       "       'station be speech a late the in punished laws if park had',\n",
       "       'We accept making a reduction in price of 5% if this will help you to develop a new market for our products',\n",
       "       'The habits of highly intelligent people offer a clue as to how to do that'],\n",
       "      dtype='|S213')"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[lastMaxes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing dev/test/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# small \n",
    "X_test, Y_test = read_csv('data/test.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166\n"
     ]
    }
   ],
   "source": [
    "# small\n",
    "import pandas as pd\n",
    "dev_ratio=0.1\n",
    "total_test_num=len(X_test_indices)\n",
    "dev_num = int(dev_ratio * len(X_test_indices))\n",
    "X_dev_indices = []\n",
    "print (dev_num)\n",
    "dev_index=(np.random.random([dev_num])*total_test_num).astype(int)\n",
    "\n",
    "X_dev_indices=X_test_indices[dev_index,:]\n",
    "X_dev = X_test[dev_index]\n",
    "Y_dev = Y_test[dev_index]\n",
    "\n",
    "X_test_after_dev=np.delete(X_test,dev_index,0)\n",
    "Y_test_after_dev=np.delete(Y_test,dev_index,0)\n",
    "\n",
    "# test after dev\n",
    "test_after_dev={'X': X_test_after_dev, 'Y': Y_test_after_dev}\n",
    "test_after_dev = pd.DataFrame(test_after_dev)\n",
    "test_after_dev.to_csv('test_minus_dev.csv',header=False,index=False)\n",
    "\n",
    "# dev\n",
    "dev={'X': X_dev, 'Y': Y_dev}\n",
    "dev = pd.DataFrame(dev)\n",
    "dev.to_csv('dev.csv',header=False,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166\n"
     ]
    }
   ],
   "source": [
    "# big\n",
    "X_test, Y_test = read_csv('data/test-big.csv')\n",
    "\n",
    "# small\n",
    "import pandas as pd\n",
    "dev_ratio=0.1\n",
    "total_test_num=len(X_test_indices)\n",
    "dev_num = int(dev_ratio * len(X_test_indices))\n",
    "X_dev_indices = []\n",
    "print (dev_num)\n",
    "dev_index=(np.random.random([dev_num])*total_test_num).astype(int)\n",
    "\n",
    "X_dev_indices=X_test_indices[dev_index,:]\n",
    "X_dev = X_test[dev_index]\n",
    "Y_dev = Y_test[dev_index]\n",
    "\n",
    "X_test_after_dev=np.delete(X_test,dev_index,0)\n",
    "Y_test_after_dev=np.delete(Y_test,dev_index,0)\n",
    "\n",
    "# test after dev\n",
    "test_after_dev={'X': X_test_after_dev, 'Y': Y_test_after_dev}\n",
    "test_after_dev = pd.DataFrame(test_after_dev)\n",
    "test_after_dev.to_csv('test_minus_dev_big.csv',header=False,index=False)\n",
    "\n",
    "# dev\n",
    "dev={'X': X_dev, 'Y': Y_dev}\n",
    "dev = pd.DataFrame(dev)\n",
    "dev.to_csv('dev_big.csv',header=False,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9908\n"
     ]
    }
   ],
   "source": [
    "# big\n",
    "X_train, Y_train = read_csv('data/train-big.csv')\n",
    "\n",
    "# small\n",
    "import pandas as pd\n",
    "ratio=0.1\n",
    "total_train_num=len(X_train)\n",
    "train_num = int(ratio * total_train_num)\n",
    "print (train_num)\n",
    "train_index=(np.random.random([train_num])*total_train_num).astype(int)\n",
    "\n",
    "X_train=X_train[train_index]\n",
    "Y_train = Y_train[train_index]\n",
    "\n",
    "# train \n",
    "X_train_shrink={'X': X_train, 'Y': Y_train}\n",
    "X_train_shrink = pd.DataFrame(X_train_shrink)\n",
    "X_train_shrink.to_csv('train_shrink.csv',sep=',',header=False,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n"
     ]
    }
   ],
   "source": [
    "maxLen = len(max(X_train, key=len).split())\n",
    "print(maxLen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the accuracy history during the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_acc_percentage=np.array(model_fitting.history['acc'][:20])*100\n",
    "history_dev_percentage=np.array(model_fitting.history['val_acc'][:20])*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAoMAAAGsCAYAAACmfK3wAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nlAAAAAlJREFUItKG/X+3HR1L+n56AgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x422676a10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.gcf()\n",
    "fig.set_size_inches(10.5, 6.5)\n",
    "\n",
    "word_train,=plt.plot(history_acc_percentage, linewidth=2,color='blue',linestyle='-',label='Word model train acc')\n",
    "word_dev,=plt.plot(history_dev_percentage, linewidth=2,color='blue',linestyle='--',label='Word model dev acc')\n",
    "plt.xlabel('# of Epoch',fontsize=16)\n",
    "plt.ylabel('Accuracy (%)',fontsize=16)\n",
    "plt.title('Accuracy Improvement',fontsize=16)\n",
    "plt.legend(handles=[word_train,word_dev],fontsize=12,loc=4)\n",
    "\n",
    "plt.show()\n",
    "fig.savefig('test2png.png', dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4914"
      ]
     },
     "execution_count": 501,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
